{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SauravTelge/eecs504/blob/main/EECS442_504_PS4_FA_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix5dQS2rUMlu"
      },
      "source": [
        "#EECS 442/504 PS4: Backpropagation\n",
        "\n",
        "__Please provide the following information__\n",
        "(e.g. Andrew Owens, ahowens):\n",
        "\n",
        "Saurav Telge, sauravt\n",
        "\n",
        "__Important__: after you download the .ipynb file, please name it as __\"PS\\<this_ps_number\\>_\\<your_uniqname\\>.ipynb\"__ before you submit it to canvas. Example: adam_01101100.ipynb.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Cst4k4tuBc"
      },
      "source": [
        "# Starting\n",
        "\n",
        "Run the following code to import the modules you'll need. After your finish the assignment, remember to run all cells and save the note book to your local machine as a .ipynb file for Canvas submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SHumIO-xt57H"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import math\n",
        "from torchvision.datasets import CIFAR10\n",
        "download = not os.path.isdir('cifar-10-batches-py')\n",
        "dset_train = CIFAR10(root='.', download=download)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "87aUvJJ52FeY"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WpKb7SvKR6W"
      },
      "source": [
        "# Problem 4.1 Understanding Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy-7c_kJKUCd"
      },
      "source": [
        "# 4.1 (b)  \n",
        "Implement the code for forward and backward pass of computation graph in (a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yojDcdIzXcNB"
      },
      "outputs": [],
      "source": [
        "def f_1(x0, x1, x2, w0, w1, w2, w3):\n",
        "    \"\"\"\n",
        "    Computes the forward and backward pass through the computational graph \n",
        "    of (a)\n",
        "\n",
        "    Inputs:\n",
        "    - x0, x1, x2, w0, w1, w2, w3: Python floats\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - L: The output of the graph\n",
        "    - grads: A tuple (grad_x0, grad_x1, grad_x2, grad_w0, grad_w1, grad_w2, \n",
        "      grad_w3)\n",
        "    giving the derivative of the output L with respect to each input.\n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the forward pass for the computational graph for (a) and#\n",
        "    # store the output of this graph as L                                     #\n",
        "    ###########################################################################\n",
        "  \n",
        "    t1= w2/x2\n",
        "    t2=w1*x1\n",
        "    t3=w0*x0\n",
        "    L = 1/(1+math.exp(-(t3+t2-t1+w3)))\n",
        "\n",
        "    ###########################################################################\n",
        "    #                              END OF YOUR CODE                           #\n",
        "    ###########################################################################\n",
        "    \n",
        "    ###########################################################################\n",
        "    # TODO: Implement the backward pass for the computational graph for (a)   #\n",
        "    # Store the gradients for each input                                      #\n",
        "    ###########################################################################\n",
        "    grad_l=1\n",
        "    grad_t3= grad_l * (math.exp(-t3))/(1+math.exp(-t3))**2 \n",
        "    grad_t2 = grad_l * (math.exp(-t2))/(1+math.exp(-t2))**2 \n",
        "    grad_t1 = grad_l * (-math.exp(t1))/(1+math.exp(t1))**2 \n",
        "    grad_x0= grad_t3* w0\n",
        "    grad_x1= grad_t2 * w1 \n",
        "    grad_x2= -w2/(x2**2) * grad_t1\n",
        "    grad_w0= grad_t3 * x0\n",
        "    grad_w1 = grad_t2 * x1\n",
        "    grad_w2 =(1/x2) * grad_t1 \n",
        "    grad_w3= grad_l * (math.exp(-w3))/(1+math.exp(-w3))**2 \n",
        "\n",
        "    ###########################################################################\n",
        "    #                              END OF YOUR CODE                           #\n",
        "    ###########################################################################\n",
        "\n",
        "    grads = (grad_x0, grad_x1, grad_x2, grad_w0, grad_w1, grad_w2, grad_w3)\n",
        "    return L, grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdquTNqGKYcc"
      },
      "source": [
        "# 4.1 (c)  \n",
        "Implement the code for forward and backward pass of computation graph in (c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o55wTks0KaPC"
      },
      "outputs": [],
      "source": [
        "def f_2(w, x, y, z):\n",
        "    \"\"\"\n",
        "    Computes the forward and backward pass through the computational graph \n",
        "    of (c)\n",
        "\n",
        "    Inputs:\n",
        "    - w, x, y, z: Python floats\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - L: The output of the graph\n",
        "    - grads: A tuple (grad_w, grad_x, grad_y, grad_z)\n",
        "    giving the derivative of the output L with respect to each input.\n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the forward pass for the computational graph for (c) and#\n",
        "    # store the output of this graph as L                                     #\n",
        "    ###########################################################################\n",
        "   \n",
        "    a = 1/w\n",
        "    b=-x\n",
        "    e=a**b\n",
        "    c=math.exp(y)\n",
        "    d= math.exp(z)\n",
        "    p=c*d\n",
        "    f=c+p\n",
        "    g=d/p\n",
        "    m=e-f\n",
        "    n=m/g\n",
        "    L=n**2\n",
        "    ###########################################################################\n",
        "    #                              END OF YOUR CODE                           #\n",
        "    ###########################################################################\n",
        "\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the backward pass for the computational graph for (c)   #\n",
        "    # Store the gradients for each input                                      #\n",
        "    ###########################################################################\n",
        "    grad_l = 1\n",
        "    grad_n = 2*n*grad_l\n",
        "    grad_m=(1/g)*grad_n\n",
        "    grad_g = (-m/g**2 )* grad_n\n",
        "    grad_e = grad_m * 1\n",
        "    grad_f = grad_m * -1\n",
        "    grad_d1=grad_g * 1/p\n",
        "    grad_p=grad_g* -d/(p**2)\n",
        "    grad_c1=grad_f * 1\n",
        "    grad_p = grad_f * 1\n",
        "    grad_c2 = grad_p*d\n",
        "    grad_d2 = grad_p*c\n",
        "    grad_c = grad_c1 + grad_c2\n",
        "    grad_d = grad_d1 + grad_d2\n",
        "    grad_z= grad_d * math.exp(z)\n",
        "    grad_y = grad_c * np.exp(y)\n",
        "    grad_a= grad_e * b*(a**(b-1))\n",
        "    grad_b= grad_e * (a**b) * np.log(a)\n",
        "    grad_x = grad_b * -1\n",
        "    grad_w = grad_a * (-1/(w**2))\n",
        "\n",
        "    ###########################################################################\n",
        "    #                              END OF YOUR CODE                           #\n",
        "    ###########################################################################\n",
        "\n",
        "    grads = (grad_w, grad_x, grad_y, grad_z)\n",
        "    return L, grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apEPzDNtK0MC"
      },
      "source": [
        "# Problem 4.2 Softmax Classifier with Two Layer Neural Network\n",
        "In this problem you will develop a two Layer neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
        "\n",
        "We train the network with a softmax loss function on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture:\n",
        "\n",
        "input - fully connected layer - ReLU - fully connected layer - softmax\n",
        "\n",
        "The outputs of the second fully-connected layer are the scores for each class.\n",
        "\n",
        "You cannot use any deep learning libraries such as PyTorch in this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXfumCQ21JoK"
      },
      "source": [
        "# 4.2 (a) Layers\n",
        "In this problem, implement fully connected layer, relu and softmax. Filling in all TODOs in skeleton codes will be sufficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q-ljfgMv9PHx"
      },
      "outputs": [],
      "source": [
        "def fc_forward(X, W, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a fully-connected layer.\n",
        "    \n",
        "    The input X has shape (N, Din) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (Din,).\n",
        "    \n",
        "    Inputs:\n",
        "    - X: A numpy array containing input data, of shape (N, Din)\n",
        "    - W: A numpy array of weights, of shape (Din, Dout)\n",
        "    - b: A numpy array of biases, of shape (Dout,)\n",
        "    \n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, Dout)\n",
        "    - cache: (X, W, b)\n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the forward pass. Store the result in out.              #\n",
        "    ###########################################################################\n",
        "\n",
        "    out = np.matmul(X, W) + b\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (X, W, b)\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def fc_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a fully_connected layer.\n",
        "    \n",
        "    Inputs:\n",
        "    - dout: Upstream derivative, of shape (N, Dout)\n",
        "    - cache: returned by your forward function. Tuple of:\n",
        "      - X: Input data, of shape (N, Din)\n",
        "      - W: Weights, of shape (Din, Dout)\n",
        "      - b: Biases, of shape (Dout,)\n",
        "      \n",
        "    Returns a tuple of:\n",
        "    - dX: Gradient with respect to X, of shape (N, Din)\n",
        "    - dW: Gradient with respect to W, of shape (Din, Dout)\n",
        "    - db: Gradient with respect to b, of shape (Dout,)\n",
        "    \"\"\"\n",
        "    X, W, b = cache\n",
        "    dX, dW, db = None, None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine backward pass.                               #\n",
        "    ###########################################################################\n",
        "\n",
        "   \n",
        "    dX = np.matmul(dout,W.T)\n",
        "    dW = np.matmul(X.T, dout )\n",
        "    db = np.sum(dout, axis=0)\n",
        "\n",
        "\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dX, dW, db\n",
        "\n",
        "def relu_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - x: Inputs, of any shape\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output, of the same shape as x\n",
        "    - cache: x\n",
        "    \"\"\"\n",
        "    out = x.copy()\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU forward pass.                                  #\n",
        "    ###########################################################################\n",
        "    \n",
        "    \n",
        "    out[out < 0] = 0\n",
        "\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = x\n",
        "\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: returned by your forward function. Input x, of same shape as dout\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx, x = dout.copy(), cache\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU backward pass.                                 #\n",
        "    ###########################################################################\n",
        "    \n",
        "  \n",
        "    dx[x<=0]=0\n",
        "\n",
        "    \n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx\n",
        "\n",
        "\n",
        "def softmax_loss(X, y):\n",
        "    \"\"\"\n",
        "    Computes the loss and gradient for softmax classification.\n",
        "\n",
        "    Inputs:\n",
        "    - X: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
        "      class for the ith input.\n",
        "    - y: Vector of labels, of shape (N,) where y[i] is the label for X[i] and\n",
        "      0 <= y[i] < C\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss: Scalar giving the loss\n",
        "    - dX: Gradient of the loss with respect to x\n",
        "    \"\"\"\n",
        "    loss, dX = None, None\n",
        "\n",
        "    dX = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "    dX /= np.sum(dX, axis=1, keepdims=True)\n",
        "    loss = -np.sum(np.log(dX[np.arange(X.shape[0]), y])) / X.shape[0]\n",
        "    dX[np.arange(X.shape[0]), y] -= 1\n",
        "    dX /= X.shape[0]\n",
        "\n",
        "\n",
        "    return loss, dX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbFxtS3zK8oz"
      },
      "source": [
        "# 4.2 (b) Two Layer Softmax Classifier\n",
        "\n",
        "In this problem, implement two layer softmax classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ytvxbx9UpxVL"
      },
      "outputs": [],
      "source": [
        "class SoftmaxClassifier(object):\n",
        "    \"\"\"\n",
        "    A fully-connected neural network with\n",
        "    softmax loss that uses a modular layer design. We assume an input dimension\n",
        "    of D, a hidden dimension of H, and perform classification over C classes.\n",
        "\n",
        "    The architecture should be fc - relu - fc - softmax with one hidden layer\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to numpy arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=3072, hidden_dim=300, num_classes=10,\n",
        "                 weight_scale=1e-3, reg=0.0):\n",
        "        \"\"\"\n",
        "        Initialize a new network.\n",
        "\n",
        "        Inputs:f\n",
        "        - input_dim: An integer giving the size of the input\n",
        "        - hidden_dim: An integer giving the size of the hidden layer, None\n",
        "          if there's no hidden layer.\n",
        "        - num_classes: An integer giving the number of classes to classify\n",
        "        - weight_scale: Scalar giving the standard deviation for random\n",
        "          initialization of the weights.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        self.reg = reg\n",
        "        ############################################################################\n",
        "        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
        "        # should be initialized from a Gaussian centered at 0.0 with               #\n",
        "        # standard deviation equal to weight_scale, and biases should be           #\n",
        "        # initialized to zero. All weights and biases should be stored in the      #\n",
        "        # dictionary self.params, with fc weights and biases using the keys        #\n",
        "        # 'W' and 'b', i.e., W1, b1 for the weights and bias in the first linear   #\n",
        "        # layer, W2, b2 for the weights and bias in the second linear layer.       #\n",
        "        ############################################################################\n",
        "        \n",
        "        self.params['W1']= np.random.normal(loc=0.0, scale=weight_scale, size=(input_dim,hidden_dim ))\n",
        "        self.params['W2']= np.random.normal(loc=0.0, scale=weight_scale, size=(hidden_dim,num_classes ))\n",
        "        self.params['b1']= np.zeros((1,hidden_dim))\n",
        "        self.params['b2']= np.zeros((1,num_classes))\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "\n",
        "    def forwards_backwards(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute loss and gradient for a minibatch of data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Array of input data of shape (N, Din)\n",
        "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
        "\n",
        "        Returns:\n",
        "        If y is None, then run a test-time forward pass of the model and return:\n",
        "        - scores: Array of shape (N, C) giving classification scores, where\n",
        "          scores[i, c] is the classification score for X[i] and class c.\n",
        "\n",
        "        If y is not None, then run a training-time forward and backward pass. And\n",
        "        return a tuple of:\n",
        "        - loss: Scalar value giving the loss\n",
        "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
        "          names to gradients of the loss with respect to those parameters.\n",
        "        \"\"\"\n",
        "        scores = None\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
        "        # class scores for X and storing them in the scores variable.              #\n",
        "        ############################################################################\n",
        "\n",
        "        out_1, cache_1 = fc_forward(X, self.params['W1'], self.params['b1'])\n",
        "        out_2,cache_2=relu_forward(out_1)\n",
        "        out_3, cache_3 = fc_forward(out_2, self.params['W2'], self.params['b2'])\n",
        "        scores = out_3\n",
        "       \n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        # If y is None then we are in test mode so just return scores\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        loss, grads = 0, {}\n",
        "        ############################################################################\n",
        "        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
        "        # in the loss variable and gradients in the grads dictionary. Compute data #\n",
        "        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
        "        # self.params[k].                                                          # \n",
        "        ############################################################################\n",
        "\n",
        "        loss, dscores = softmax_loss(scores, y)\n",
        "  \n",
        "\n",
        "        dx_2, grads['W2'], grads['b2'] = fc_backward(dscores, cache_3)\n",
        "        dx_2=relu_backward(dx_2,cache_2)\n",
        "        dx_1, grads['W1'], grads['b1'] = fc_backward(dx_2, cache_1)\n",
        "\n",
        "        grads['W2'] += self.reg*self.params['W2']\n",
        "        grads['W1'] += self.reg*self.params['W1']\n",
        "\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "        ############################################################################\n",
        "        # TODO: 4.2(g)(EECS 504 only) Add L2 regularization                        # \n",
        "        ############################################################################\n",
        "\n",
        "        loss += 0.5*self.reg*np.sum(self.params['W1']**2) + 0.5*self.reg*np.sum(self.params['W2']**2)\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "        return loss, grads\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwp0waIL1h_e"
      },
      "source": [
        "# 4.2(c) Training\n",
        "\n",
        "In this problem, you need to preprocess the images and set up model hyperparameters. Notice that adjust the training and val split is optional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kZPtQzXGMoCg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a8768f4-9cb5-47df-b442-2ae5233f7319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Iteration 1 / 6250) loss: 2.308494\n",
            "(Epoch 0 / 10) train acc: 0.160000; val_acc: 0.159500\n",
            "(Epoch 1 / 10) train acc: 0.415000; val_acc: 0.408200\n",
            "(Iteration 1001 / 6250) loss: 1.272120\n",
            "(Epoch 2 / 10) train acc: 0.528000; val_acc: 0.478900\n",
            "(Epoch 3 / 10) train acc: 0.531000; val_acc: 0.479400\n",
            "(Iteration 2001 / 6250) loss: 1.280092\n",
            "(Epoch 4 / 10) train acc: 0.512000; val_acc: 0.479400\n",
            "(Iteration 3001 / 6250) loss: 1.478959\n",
            "(Epoch 5 / 10) train acc: 0.519000; val_acc: 0.479400\n",
            "(Epoch 6 / 10) train acc: 0.533000; val_acc: 0.479400\n",
            "(Iteration 4001 / 6250) loss: 1.342210\n",
            "(Epoch 7 / 10) train acc: 0.534000; val_acc: 0.479400\n",
            "(Epoch 8 / 10) train acc: 0.540000; val_acc: 0.479400\n",
            "(Iteration 5001 / 6250) loss: 1.379201\n",
            "(Epoch 9 / 10) train acc: 0.534000; val_acc: 0.479400\n",
            "(Iteration 6001 / 6250) loss: 1.344345\n",
            "(Epoch 10 / 10) train acc: 0.483000; val_acc: 0.479400\n"
          ]
        }
      ],
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding=\"latin1\")\n",
        "    return dict\n",
        "\n",
        "def load_cifar10():\n",
        "    data = {}\n",
        "    meta = unpickle(\"cifar-10-batches-py/batches.meta\")\n",
        "    batch1 = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
        "    batch2 = unpickle(\"cifar-10-batches-py/data_batch_2\")\n",
        "    batch3 = unpickle(\"cifar-10-batches-py/data_batch_3\")\n",
        "    batch4 = unpickle(\"cifar-10-batches-py/data_batch_4\")\n",
        "    batch5 = unpickle(\"cifar-10-batches-py/data_batch_5\")\n",
        "    test_batch = unpickle(\"cifar-10-batches-py/test_batch\")\n",
        "    X_train = np.vstack((batch1['data'], batch2['data'], batch3['data'],\\\n",
        "                         batch4['data'], batch5['data']))\n",
        "    Y_train = np.array(batch1['labels'] + batch2['labels'] + batch3['labels'] + \n",
        "                       batch4['labels'] + batch5['labels'])\n",
        "    X_test = test_batch['data']\n",
        "    Y_test = test_batch['labels']\n",
        "    \n",
        "    #Preprocess images here                                     \n",
        "    X_train = (X_train-np.mean(X_train,axis=1,keepdims=True))/np.std(X_train,axis=1,keepdims=True)\n",
        "    X_test = (X_test-np.mean(X_test,axis=1,keepdims=True))/np.std(X_test,axis=1,keepdims=True)\n",
        "\n",
        "    data['X_train'] = X_train[:40000]\n",
        "    data['y_train'] = Y_train[:40000]\n",
        "    data['X_val'] = X_train[40000:]\n",
        "    data['y_val'] = Y_train[40000:]\n",
        "    data['X_test'] = X_test\n",
        "    data['y_test'] = Y_test\n",
        "    return data\n",
        "\n",
        "def testNetwork(model, X, y, num_samples=None, batch_size=100):\n",
        "    \"\"\"\n",
        "    Check accuracy of the model on the provided data.\n",
        "\n",
        "    Inputs:\n",
        "    - model: Image classifier\n",
        "    - X: Array of data, of shape (N, d_1, ..., d_k)\n",
        "    - y: Array of labels, of shape (N,)\n",
        "    - num_samples: If not None, subsample the data and only test the model\n",
        "      on num_samples datapoints.\n",
        "    - batch_size: Split X and y into batches of this size to avoid using\n",
        "      too much memory.\n",
        "\n",
        "    Returns:\n",
        "    - acc: Scalar giving the fraction of instances that were correctly\n",
        "      classified by the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Subsample the data\n",
        "    N = X.shape[0]\n",
        "    if num_samples is not None and N > num_samples:\n",
        "        mask = np.random.choice(N, num_samples)\n",
        "        N = num_samples\n",
        "        X = X[mask]\n",
        "        y = y[mask]\n",
        "\n",
        "    # Compute predictions in batches\n",
        "    num_batches = N // batch_size\n",
        "    if N % batch_size != 0:\n",
        "        num_batches += 1\n",
        "    y_pred = []\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = (i + 1) * batch_size\n",
        "        scores = model.forwards_backwards(X[start:end])\n",
        "        y_pred.append(np.argmax(scores, axis=1))\n",
        "    y_pred = np.hstack(y_pred)\n",
        "    acc = np.mean(y_pred == y)\n",
        "\n",
        "    return acc\n",
        "\n",
        "def SGD(W,dW, learning_rate=1e-3):\n",
        "    \"\"\" Apply a gradient descent step on weight W \n",
        "    Inputs:\n",
        "        W : Weight matrix\n",
        "        dW : gradient of weight, same shape as W\n",
        "        learning_rate : Learning rate. Defaults to 1e-3.\n",
        "    Returns:\n",
        "        new_W: Updated weight matrix\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply a gradient descent step on weight W using the gradient dW and the specified learning rate.\n",
        "    new_W = W - learning_rate * dW\n",
        "\n",
        "    return new_W\n",
        "\n",
        "def trainNetwork(model, data, **kwargs):\n",
        "    \"\"\"\n",
        "     Required arguments:\n",
        "    - model: Image classifier\n",
        "    - data: A dictionary of training and validation data containing:\n",
        "      'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
        "      'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
        "      'y_train': Array, shape (N_train,) of labels for training images\n",
        "      'y_val': Array, shape (N_val,) of labels for validation images\n",
        "\n",
        "    Optional arguments:\n",
        "    - learning_rate: A scalar for initial learning rate.\n",
        "    - lr_decay: A scalar for learning rate decay; after each epoch the\n",
        "      learning rate is multiplied by this value.\n",
        "    - batch_size: Size of minibatches used to compute loss and gradient\n",
        "      during training.\n",
        "    - num_epochs: The number of epochs to run for during training.\n",
        "    - print_every: Integer; training losses will be printed every\n",
        "      print_every iterations.\n",
        "    - verbose: Boolean; if set to false then no output will be printed\n",
        "      during training.\n",
        "    - num_train_samples: Number of training samples used to check training\n",
        "      accuracy; default is 1000; set to None to use entire training set.\n",
        "    - num_val_samples: Number of validation samples to use to check val\n",
        "      accuracy; default is None, which uses the entire validation set.\n",
        "    - optimizer: Choice of using either 'SGD' or 'SGD_Momentum' for updating weights; default is SGD.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    learning_rate =  kwargs.pop('learning_rate', 1e-3)\n",
        "    lr_decay = kwargs.pop('lr_decay', 1.0)\n",
        "    batch_size = kwargs.pop('batch_size', 100)\n",
        "    num_epochs = kwargs.pop('num_epochs', 10)\n",
        "    num_train_samples = kwargs.pop('num_train_samples', 1000)\n",
        "    num_val_samples = kwargs.pop('num_val_samples', None)\n",
        "    print_every = kwargs.pop('print_every', 10)   \n",
        "    verbose = kwargs.pop('verbose', True)\n",
        "    optimizer = kwargs.pop('optimizer', 'SGD')\n",
        "    \n",
        "    epoch = 0\n",
        "    best_val_acc = 0\n",
        "    best_params = {}\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    \n",
        "    \n",
        "    num_train = data['X_train'].shape[0]\n",
        "    iterations_per_epoch = max(num_train // batch_size, 1)\n",
        "    num_iterations = num_epochs * iterations_per_epoch\n",
        "    \n",
        "    #Initialize velocity dictionary if optimizer is SGD_Momentum\n",
        "    if optimizer == 'SGD_Momentum':\n",
        "      velocity_dict = {p:np.zeros(w.shape) for p,w in model.params.items()}\n",
        "      \n",
        "    for t in range(num_iterations): \n",
        "        # Make a minibatch of training data\n",
        "        batch_mask = np.random.choice(num_train, batch_size)\n",
        "        X_batch = data['X_train'][batch_mask]\n",
        "        y_batch = data['y_train'][batch_mask]\n",
        "        \n",
        "        # Compute loss and gradient\n",
        "        loss, grads = model.forwards_backwards(X_batch, y_batch)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Perform a parameter update\n",
        "        if optimizer == 'SGD':\n",
        "          for p, w in model.params.items():\n",
        "              model.params[p] = SGD(w,grads[p], learning_rate=learning_rate)\n",
        "\n",
        "        elif optimizer == 'SGD_Momentum':\n",
        "          for p, w in model.params.items():\n",
        "              model.params[p], velocity_dict[p] = SGD_Momentum(w, grads[p], velocity_dict[p], beta=0.5, learning_rate=learning_rate)\n",
        "        else:\n",
        "          raise NotImplementedError\n",
        "        # Print training loss\n",
        "        if verbose and t % print_every == 0:\n",
        "            print('(Iteration %d / %d) loss: %f' % (\n",
        "                   t + 1, num_iterations, loss_history[-1]))\n",
        "         \n",
        "        # At the end of every epoch, increment the epoch counter and decay\n",
        "        # the learning rate.\n",
        "        epoch_end = (t + 1) % iterations_per_epoch == 0\n",
        "        if epoch_end:\n",
        "            epoch += 1\n",
        "            learning_rate *= lr_decay\n",
        "        \n",
        "        # Check train and val accuracy on the first iteration, the last\n",
        "        # iteration, and at the end of each epoch.\n",
        "        first_it = (t == 0)\n",
        "        last_it = (t == num_iterations - 1)\n",
        "        if first_it or last_it or epoch_end:\n",
        "            train_acc = testNetwork(model, data['X_train'], data['y_train'],\n",
        "                num_samples= num_train_samples)\n",
        "            val_acc = testNetwork(model, data['X_val'], data['y_val'],\n",
        "                num_samples=num_val_samples)\n",
        "            train_acc_history.append(train_acc)\n",
        "            val_acc_history.append(val_acc)\n",
        "\n",
        "            if verbose:\n",
        "                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
        "                       epoch, num_epochs, train_acc, val_acc))\n",
        "\n",
        "            # Keep track of the best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_params = {}\n",
        "                for k, v in model.params.items():\n",
        "                    best_params[k] = v.copy()\n",
        "        \n",
        "    model.params = best_params\n",
        "        \n",
        "    return model, train_acc_history, val_acc_history\n",
        "        \n",
        "\n",
        "# load data\n",
        "data = load_cifar10() \n",
        "train_data = { k: data[k] for k in ['X_train', 'y_train', \n",
        "                                    'X_val', 'y_val']}\n",
        "\n",
        "# initialize model\n",
        "model_SGD = SoftmaxClassifier(hidden_dim = 300, weight_scale=1e-2)\n",
        "\n",
        "#######################################################################\n",
        "# TODO: Set up model hyperparameters for SGD                          #\n",
        "#######################################################################\n",
        "\n",
        "# set the hyperparameter\n",
        "learning_rate_SGD = 0.1\n",
        "lr_decay_SGD =learning_rate_SGD / 10\n",
        "batch_size_SGD = 64\n",
        "\n",
        "# start training using SGD\n",
        "model_SGD, train_acc_history_SGD, val_acc_history_SGD = trainNetwork(\n",
        "    model_SGD, train_data, \n",
        "    learning_rate = learning_rate_SGD,\n",
        "    lr_decay=lr_decay_SGD, \n",
        "    batch_size=batch_size_SGD,\n",
        "    num_epochs=10, \n",
        "    print_every=1000, optimizer = 'SGD')\n",
        "#######################################################################\n",
        "#                         END OF YOUR CODE                            #\n",
        "#######################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2ilTVXIw_7q"
      },
      "source": [
        "# 4.2(d) Training with SGD_Momentum\n",
        "\n",
        "The model above was trained using SGD. Now implement the SGD with momentum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "54jGVPZOXtV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd3d4e2c-b320-44e6-f52c-3f5660af267e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Iteration 1 / 6250) loss: 2.281983\n",
            "(Epoch 0 / 10) train acc: 0.162000; val_acc: 0.157800\n",
            "(Epoch 1 / 10) train acc: 0.378000; val_acc: 0.363100\n",
            "(Iteration 1001 / 6250) loss: 1.695751\n",
            "(Epoch 2 / 10) train acc: 0.492000; val_acc: 0.459300\n",
            "(Epoch 3 / 10) train acc: 0.494000; val_acc: 0.459700\n",
            "(Iteration 2001 / 6250) loss: 1.413757\n",
            "(Epoch 4 / 10) train acc: 0.499000; val_acc: 0.459700\n",
            "(Iteration 3001 / 6250) loss: 1.477476\n",
            "(Epoch 5 / 10) train acc: 0.499000; val_acc: 0.459700\n",
            "(Epoch 6 / 10) train acc: 0.518000; val_acc: 0.459700\n",
            "(Iteration 4001 / 6250) loss: 1.737962\n",
            "(Epoch 7 / 10) train acc: 0.530000; val_acc: 0.459700\n",
            "(Epoch 8 / 10) train acc: 0.507000; val_acc: 0.459700\n",
            "(Iteration 5001 / 6250) loss: 1.978494\n",
            "(Epoch 9 / 10) train acc: 0.508000; val_acc: 0.459700\n",
            "(Iteration 6001 / 6250) loss: 1.337744\n",
            "(Epoch 10 / 10) train acc: 0.508000; val_acc: 0.459700\n"
          ]
        }
      ],
      "source": [
        "def SGD_Momentum(W, dW, velocity, beta=0.5, learning_rate=1e-3):\n",
        "    \"\"\" Apply a gradient descent with momentum update on weight W\n",
        "    Inputs:\n",
        "        W : Weight matrix\n",
        "        dW : gradient of weight, same shape as W\n",
        "        velocity : velocity matrix, same shape as W\n",
        "        beta : scalar value in range [0,1] weighting the velocity matrix. Setting it to 0 should make SGD_Momentum same as SGD. \n",
        "               Defaults to 0.5.\n",
        "        learning_rate : Learning rate. Defaults to 1e-3.\n",
        "    Returns:\n",
        "        new_W: Updated weight matrix\n",
        "        new_velocity: Updated velocity matrix\n",
        "    \"\"\"\n",
        "    #######################################################################\n",
        "    # TODO: Apply a gradient descent step on weight W using the gradient dW and the specified learning rate.\n",
        "    # 1. Calculate the new velocity by using the velocity of last iteration (input velocity) and gradient\n",
        "    # 2. Update the weights using the new_velocity\n",
        "    #######################################################################\n",
        "\n",
        "    new_velocity = beta*velocity + learning_rate*dW\n",
        "    new_W = W - new_velocity\n",
        "\n",
        "\n",
        "\n",
        "    #######################################################################\n",
        "    #                         END OF YOUR CODE                            #\n",
        "    #######################################################################\n",
        "    return new_W, new_velocity\n",
        "\n",
        "# initialize model\n",
        "model_SGD_Momentum = SoftmaxClassifier(hidden_dim = 300, weight_scale=1e-2)\n",
        "\n",
        "# start training \n",
        "#Using SGD_Momentum as optimizer for trainning for training\n",
        "model_SGD_Momentum, train_acc_history_SGD_Momentum, val_acc_history_SGD_Momentum = trainNetwork(\n",
        "    model_SGD_Momentum, train_data, \n",
        "    learning_rate = learning_rate_SGD,\n",
        "    lr_decay=lr_decay_SGD, \n",
        "    batch_size=batch_size_SGD,\n",
        "    num_epochs=10, \n",
        "    print_every=1000, optimizer = 'SGD_Momentum')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcovGmpXvXXa"
      },
      "source": [
        "# 4.2(e) Report Accuracy\n",
        "\n",
        "Run the given code and report the accuracy of model_SGD and model_SGD_Momentum on test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwCq8pBhu6dz",
        "outputId": "a9d01fc6-05e3-4e67-fa8b-5ad6716792ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy of model_SGD: 0.4783\n",
            "Test accuracy of model_SGD_Momentum: 0.4595\n"
          ]
        }
      ],
      "source": [
        "# report test accuracy\n",
        "acc = testNetwork(model_SGD, data['X_test'], data['y_test'])\n",
        "print(\"Test accuracy of model_SGD: {}\".format(acc))\n",
        "\n",
        "# report test accuracy\n",
        "acc = testNetwork(model_SGD_Momentum, data['X_test'], data['y_test'])\n",
        "print(\"Test accuracy of model_SGD_Momentum: {}\".format(acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTrmbULS7i2N"
      },
      "source": [
        "# 4.2(f) Plot\n",
        "\n",
        "Using the train_acc_history and val_acc_history, plot the train & val accuracy versus epochs on one plot, using SGD and SGD_Momentum as optimizer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SPjtnbya9S7g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "225bb85e-39c2-496a-cbd3-8d318658ebd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Traning Loss vs epochs')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#######################################################################\n",
        "# Your Code here\n",
        "#######################################################################\n",
        "plt.plot(np.arange(11),train_acc_history_SGD,color='b',label='SGD')\n",
        "plt.plot(np.arange(11),train_acc_history_SGD_Momentum,color='r',label='SGD_Momentum')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.legend()\n",
        "plt.title('Traning Loss vs epochs')\n",
        "#######################################################################\n",
        "#                         END OF YOUR CODE                            #\n",
        "#######################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Report your obervation here:"
      ],
      "metadata": {
        "id": "GVcoULZa5seR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxOkNE5IE76I"
      },
      "source": [
        "# 4.2(g) Adding L2 regularization (EECS 504 Only)\n",
        "\n",
        "Add L2 regularization to the softmax classifier in 4.2(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2(h) Training with L2 regularization (EECS 504 Only)\n",
        "\n",
        "Train the model again using L2 regularization, using SGD"
      ],
      "metadata": {
        "id": "e3VuqVbKh6VB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tQoj9kabUBy8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cff0c6e4-33e2-44db-b9de-feb3c293a079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Iteration 1 / 6250) loss: 2.760004\n",
            "(Epoch 0 / 10) train acc: 0.170000; val_acc: 0.164500\n",
            "(Epoch 1 / 10) train acc: 0.481000; val_acc: 0.426200\n",
            "(Iteration 1001 / 6250) loss: 1.617095\n",
            "(Epoch 2 / 10) train acc: 0.535000; val_acc: 0.464300\n",
            "(Epoch 3 / 10) train acc: 0.556000; val_acc: 0.464600\n",
            "(Iteration 2001 / 6250) loss: 1.768689\n",
            "(Epoch 4 / 10) train acc: 0.502000; val_acc: 0.464600\n",
            "(Iteration 3001 / 6250) loss: 1.596591\n",
            "(Epoch 5 / 10) train acc: 0.529000; val_acc: 0.464600\n",
            "(Epoch 6 / 10) train acc: 0.513000; val_acc: 0.464600\n",
            "(Iteration 4001 / 6250) loss: 1.614995\n",
            "(Epoch 7 / 10) train acc: 0.508000; val_acc: 0.464600\n",
            "(Epoch 8 / 10) train acc: 0.508000; val_acc: 0.464600\n",
            "(Iteration 5001 / 6250) loss: 1.656293\n",
            "(Epoch 9 / 10) train acc: 0.527000; val_acc: 0.464600\n",
            "(Iteration 6001 / 6250) loss: 1.525632\n",
            "(Epoch 10 / 10) train acc: 0.512000; val_acc: 0.464600\n"
          ]
        }
      ],
      "source": [
        "# initialize model (remember to set the l2 regularization weight > 0)\n",
        "model_SGD_L2 = SoftmaxClassifier(hidden_dim = 300, weight_scale=1e-2, reg=0.01)\n",
        "\n",
        "# start training using SGD. The training hyperparameter you choose should be same to the 4.2(c)\n",
        "model_SGD_L2, train_acc_history_SGD_L2, val_acc_history_SGD_L2 = trainNetwork(\n",
        "    model_SGD_L2, train_data, \n",
        "    learning_rate = learning_rate_SGD,\n",
        "    lr_decay=lr_decay_SGD, \n",
        "    batch_size=batch_size_SGD,\n",
        "    num_epochs=10, \n",
        "    print_every=1000, optimizer = 'SGD')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "R5yYAg3mWFX5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10829884-af96-49bd-91a1-650f5c70256b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy of model_SGD_L2: 0.4663\n"
          ]
        }
      ],
      "source": [
        "# report test accuracy\n",
        "acc = testNetwork(model_SGD_L2, data['X_test'], data['y_test'])\n",
        "print(\"Test accuracy of model_SGD_L2: {}\".format(acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2(i) Plot\n",
        "\n",
        "Using the train_acc_history and val_acc_history, plot the train & val accuracy versus epochs on one plot for model with and without L2 regularization, using SGD as optimizer. "
      ],
      "metadata": {
        "id": "9AuWH2_Aj2Ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################################\n",
        "# TODO: Your Code here\n",
        "#######################################################################\n",
        "plt.plot(np.arange(11),train_acc_history_SGD,color='b',label='SGD')\n",
        "plt.plot(np.arange(11),train_acc_history_SGD_L2,color='r',label='SGD_Momentum')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.legend()\n",
        "plt.title('Traning Loss vs epochs')\n",
        "\n",
        "#######################################################################\n",
        "#                         END OF YOUR CODE                            #\n",
        "#######################################################################"
      ],
      "metadata": {
        "id": "eG7H8nXCgudY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "8f28a4a0-c4f0-43f6-c35e-50005a6e50b1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Traning Loss vs epochs')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2(j) Tune your own model\n",
        "\n",
        "Feel free to tune any hyperparameters and choose any optimizer as you want -- just train the best model you can!"
      ],
      "metadata": {
        "id": "MbrfhTzdlyRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################################\n",
        "# TODO: Train your own model                                          #\n",
        "#######################################################################\n",
        "\n",
        "# initialize model (set reg=0.0 for EECS 442 students)\n",
        "# input_dim=3072, hidden_dim=300, num_classes=10,\n",
        "#                  weight_scale=1e-3, reg=0.0\n",
        "your_model = SoftmaxClassifier(hidden_dim = 500, weight_scale=1e-2,  reg= 0.001)\n",
        "\n",
        "# train your moodel\n",
        "your_model, train_acc_history_your_model, val_acc_history_your_model = trainNetwork(\n",
        "    your_model, train_data, \n",
        "    learning_rate = 0.1 ,\n",
        "    lr_decay=  0.1 , \n",
        "    batch_size= 128,\n",
        "    num_epochs= 10 , \n",
        "    print_every=1000, optimizer = 'SGD_Momentum')\n",
        "#######################################################################\n",
        "#                         END OF YOUR CODE                            #\n",
        "#######################################################################"
      ],
      "metadata": {
        "id": "SrjE2jyKmY-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3646e46-93fc-42f3-8f46-d575e633b236"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Iteration 1 / 3120) loss: 2.374129\n",
            "(Epoch 0 / 10) train acc: 0.179000; val_acc: 0.168800\n",
            "(Epoch 1 / 10) train acc: 0.471000; val_acc: 0.418200\n",
            "(Epoch 2 / 10) train acc: 0.553000; val_acc: 0.508600\n",
            "(Epoch 3 / 10) train acc: 0.561000; val_acc: 0.515200\n",
            "(Iteration 1001 / 3120) loss: 1.276321\n",
            "(Epoch 4 / 10) train acc: 0.604000; val_acc: 0.515200\n",
            "(Epoch 5 / 10) train acc: 0.595000; val_acc: 0.515100\n",
            "(Epoch 6 / 10) train acc: 0.593000; val_acc: 0.515100\n",
            "(Iteration 2001 / 3120) loss: 1.365748\n",
            "(Epoch 7 / 10) train acc: 0.600000; val_acc: 0.515100\n",
            "(Epoch 8 / 10) train acc: 0.579000; val_acc: 0.515100\n",
            "(Epoch 9 / 10) train acc: 0.572000; val_acc: 0.515100\n",
            "(Iteration 3001 / 3120) loss: 1.339750\n",
            "(Epoch 10 / 10) train acc: 0.577000; val_acc: 0.515100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# report test accuracy \n",
        "# (Run this code only once when you obtain the highest model in validation set!)\n",
        "acc = testNetwork(your_model, data['X_test'], data['y_test'])\n",
        "print(\"Test accuracy of your model: {}\".format(acc))"
      ],
      "metadata": {
        "id": "vI4SK0PPmbrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6027c9b0-b49d-47a8-ff07-3f30b61bf21e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy of your model: 0.5121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7MoINIIhYf7"
      },
      "source": [
        "# Convert Notebook to PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z5w3nymDr3_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d2b0e1-d7a5-4d61-e13f-711d9dc4ea77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "drive_mount_point = '/content/drive/'\n",
        "drive.mount(drive_mount_point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "38xLsgYnLkw1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c3739923-d29f-4dfd-ca0d-75baa04b6a4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.39)] [\u001b[0m\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [2 InRelease 47.5 kB/88.7 kB 54%] [Waiting for headers] [Waiting for headers\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 242 kB] [2 InRelease 47.5 kB/88.7 kB 54%] [Waiting for hea\u001b[0m\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 242 kB] [2 InRelease 79.3 kB/88.7 kB 89%] [4 InRelease 14.\u001b[0m\r                                                                               \rGet:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 242 kB] [2 InRelease 82.2 kB/88.7 kB 93%] [4 InRelease 14.\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [4 InRelease 14.2 kB/88.7 kB\u001b[0m\r                                                                               \rIgn:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [4 InRelease 14.2 kB/88.7 kB\u001b[0m\r                                                                               \rGet:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [4 InRelease 14.2 kB/88.7 kB\u001b[0m\r                                                                               \rGet:8 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,425 kB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [948 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,324 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,204 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,163 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,546 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,163 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,992 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,108 kB]\n",
            "Get:23 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [48.3 kB]\n",
            "Fetched 17.2 MB in 5s (3,222 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "32 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono fonts-texgyre\n",
            "  javascript-common libcupsfilters1 libcupsimage2 libgs9 libgs9-common\n",
            "  libijs-0.35 libjbig2dec0 libjs-jquery libkpathsea6 libpotrace0 libptexenc1\n",
            "  libruby2.5 libsynctex1 libtexlua52 libtexluajit2 libzzip-0-13 lmodern\n",
            "  poppler-data preview-latex-style rake ruby ruby-did-you-mean ruby-minitest\n",
            "  ruby-net-telnet ruby-power-assert ruby-test-unit ruby2.5\n",
            "  rubygems-integration t1utils tex-common tex-gyre texlive-base\n",
            "  texlive-binaries texlive-latex-base texlive-latex-extra\n",
            "  texlive-latex-recommended texlive-pictures texlive-plain-generic tipa\n",
            "Suggested packages:\n",
            "  fonts-noto apache2 | lighttpd | httpd poppler-utils ghostscript\n",
            "  fonts-japanese-mincho | fonts-ipafont-mincho fonts-japanese-gothic\n",
            "  | fonts-ipafont-gothic fonts-arphic-ukai fonts-arphic-uming fonts-nanum ri\n",
            "  ruby-dev bundler debhelper gv | postscript-viewer perl-tk xpdf-reader\n",
            "  | pdf-viewer texlive-fonts-recommended-doc texlive-latex-base-doc\n",
            "  python-pygments icc-profiles libfile-which-perl\n",
            "  libspreadsheet-parseexcel-perl texlive-latex-extra-doc\n",
            "  texlive-latex-recommended-doc texlive-pstricks dot2tex prerex ruby-tcltk\n",
            "  | libtcltk-ruby texlive-pictures-doc vprerex\n",
            "The following NEW packages will be installed:\n",
            "  fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono fonts-texgyre\n",
            "  javascript-common libcupsfilters1 libcupsimage2 libgs9 libgs9-common\n",
            "  libijs-0.35 libjbig2dec0 libjs-jquery libkpathsea6 libpotrace0 libptexenc1\n",
            "  libruby2.5 libsynctex1 libtexlua52 libtexluajit2 libzzip-0-13 lmodern\n",
            "  poppler-data preview-latex-style rake ruby ruby-did-you-mean ruby-minitest\n",
            "  ruby-net-telnet ruby-power-assert ruby-test-unit ruby2.5\n",
            "  rubygems-integration t1utils tex-common tex-gyre texlive-base\n",
            "  texlive-binaries texlive-fonts-recommended texlive-generic-recommended\n",
            "  texlive-latex-base texlive-latex-extra texlive-latex-recommended\n",
            "  texlive-pictures texlive-plain-generic texlive-xetex tipa\n",
            "0 upgraded, 47 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 146 MB of archives.\n",
            "After this operation, 460 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-lato all 2.0-2 [2,698 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 poppler-data all 0.4.8-2 [1,479 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 tex-common all 6.09 [33.0 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-lmodern all 2.004.5-3 [4,551 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-noto-mono all 20171026-2 [75.5 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-texgyre all 20160520-1 [8,761 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 javascript-common all 11 [6,066 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsfilters1 amd64 1.20.2-0ubuntu3.1 [108 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsimage2 amd64 2.2.7-1ubuntu2.9 [18.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libijs-0.35 amd64 0.35-13 [15.5 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjbig2dec0 amd64 0.13-6 [55.9 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9-common all 9.26~dfsg+0-0ubuntu0.18.04.17 [5,092 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9 amd64 9.26~dfsg+0-0ubuntu0.18.04.17 [2,267 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjs-jquery all 3.2.1-1 [152 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libkpathsea6 amd64 2017.20170613.44572-8ubuntu0.1 [54.9 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpotrace0 amd64 1.14-2 [17.4 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libptexenc1 amd64 2017.20170613.44572-8ubuntu0.1 [34.5 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 rubygems-integration all 1.11 [4,994 B]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ruby2.5 amd64 2.5.1-1ubuntu1.12 [48.6 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby amd64 1:2.5.1 [5,712 B]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 rake all 12.3.1-1ubuntu0.1 [44.9 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-did-you-mean all 1.2.0-2 [9,700 B]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-minitest all 5.10.3-1 [38.6 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-power-assert all 0.3.0-1 [7,952 B]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-test-unit all 3.2.5-1 [61.1 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libruby2.5 amd64 2.5.1-1ubuntu1.12 [3,073 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsynctex1 amd64 2017.20170613.44572-8ubuntu0.1 [41.4 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtexlua52 amd64 2017.20170613.44572-8ubuntu0.1 [91.2 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtexluajit2 amd64 2017.20170613.44572-8ubuntu0.1 [230 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libzzip-0-13 amd64 0.13.62-3.1ubuntu0.18.04.1 [26.0 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic/main amd64 lmodern all 2.004.5-3 [9,631 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic/main amd64 preview-latex-style all 11.91-1ubuntu1 [185 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic/main amd64 t1utils amd64 1.41-2 [56.0 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tex-gyre all 20160520-1 [4,998 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 texlive-binaries amd64 2017.20170613.44572-8ubuntu0.1 [8,179 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-base all 2017.20180305-1 [18.7 MB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-fonts-recommended all 2017.20180305-1 [5,262 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-plain-generic all 2017.20180305-2 [23.6 MB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-generic-recommended all 2017.20180305-1 [15.9 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-latex-base all 2017.20180305-1 [951 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-latex-recommended all 2017.20180305-1 [14.9 MB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-pictures all 2017.20180305-1 [4,026 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-latex-extra all 2017.20180305-2 [10.6 MB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tipa all 2:1.3-20 [2,978 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-xetex all 2017.20180305-1 [10.7 MB]\n",
            "Fetched 146 MB in 3s (53.7 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 159447 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Selecting previously unselected package fonts-lato.\n",
            "Preparing to unpack .../01-fonts-lato_2.0-2_all.deb ...\n",
            "Unpacking fonts-lato (2.0-2) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../02-poppler-data_0.4.8-2_all.deb ...\n",
            "Unpacking poppler-data (0.4.8-2) ...\n",
            "Selecting previously unselected package tex-common.\n",
            "Preparing to unpack .../03-tex-common_6.09_all.deb ...\n",
            "Unpacking tex-common (6.09) ...\n",
            "Selecting previously unselected package fonts-lmodern.\n",
            "Preparing to unpack .../04-fonts-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking fonts-lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../05-fonts-noto-mono_20171026-2_all.deb ...\n",
            "Unpacking fonts-noto-mono (20171026-2) ...\n",
            "Selecting previously unselected package fonts-texgyre.\n",
            "Preparing to unpack .../06-fonts-texgyre_20160520-1_all.deb ...\n",
            "Unpacking fonts-texgyre (20160520-1) ...\n",
            "Selecting previously unselected package javascript-common.\n",
            "Preparing to unpack .../07-javascript-common_11_all.deb ...\n",
            "Unpacking javascript-common (11) ...\n",
            "Selecting previously unselected package libcupsfilters1:amd64.\n",
            "Preparing to unpack .../08-libcupsfilters1_1.20.2-0ubuntu3.1_amd64.deb ...\n",
            "Unpacking libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Selecting previously unselected package libcupsimage2:amd64.\n",
            "Preparing to unpack .../09-libcupsimage2_2.2.7-1ubuntu2.9_amd64.deb ...\n",
            "Unpacking libcupsimage2:amd64 (2.2.7-1ubuntu2.9) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../10-libijs-0.35_0.35-13_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-13) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../11-libjbig2dec0_0.13-6_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.13-6) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../12-libgs9-common_9.26~dfsg+0-0ubuntu0.18.04.17_all.deb ...\n",
            "Unpacking libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.17) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../13-libgs9_9.26~dfsg+0-0ubuntu0.18.04.17_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.17) ...\n",
            "Selecting previously unselected package libjs-jquery.\n",
            "Preparing to unpack .../14-libjs-jquery_3.2.1-1_all.deb ...\n",
            "Unpacking libjs-jquery (3.2.1-1) ...\n",
            "Selecting previously unselected package libkpathsea6:amd64.\n",
            "Preparing to unpack .../15-libkpathsea6_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libpotrace0.\n",
            "Preparing to unpack .../16-libpotrace0_1.14-2_amd64.deb ...\n",
            "Unpacking libpotrace0 (1.14-2) ...\n",
            "Selecting previously unselected package libptexenc1:amd64.\n",
            "Preparing to unpack .../17-libptexenc1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package rubygems-integration.\n",
            "Preparing to unpack .../18-rubygems-integration_1.11_all.deb ...\n",
            "Unpacking rubygems-integration (1.11) ...\n",
            "Selecting previously unselected package ruby2.5.\n",
            "Preparing to unpack .../19-ruby2.5_2.5.1-1ubuntu1.12_amd64.deb ...\n",
            "Unpacking ruby2.5 (2.5.1-1ubuntu1.12) ...\n",
            "Selecting previously unselected package ruby.\n",
            "Preparing to unpack .../20-ruby_1%3a2.5.1_amd64.deb ...\n",
            "Unpacking ruby (1:2.5.1) ...\n",
            "Selecting previously unselected package rake.\n",
            "Preparing to unpack .../21-rake_12.3.1-1ubuntu0.1_all.deb ...\n",
            "Unpacking rake (12.3.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package ruby-did-you-mean.\n",
            "Preparing to unpack .../22-ruby-did-you-mean_1.2.0-2_all.deb ...\n",
            "Unpacking ruby-did-you-mean (1.2.0-2) ...\n",
            "Selecting previously unselected package ruby-minitest.\n",
            "Preparing to unpack .../23-ruby-minitest_5.10.3-1_all.deb ...\n",
            "Unpacking ruby-minitest (5.10.3-1) ...\n",
            "Selecting previously unselected package ruby-net-telnet.\n",
            "Preparing to unpack .../24-ruby-net-telnet_0.1.1-2_all.deb ...\n",
            "Unpacking ruby-net-telnet (0.1.1-2) ...\n",
            "Selecting previously unselected package ruby-power-assert.\n",
            "Preparing to unpack .../25-ruby-power-assert_0.3.0-1_all.deb ...\n",
            "Unpacking ruby-power-assert (0.3.0-1) ...\n",
            "Selecting previously unselected package ruby-test-unit.\n",
            "Preparing to unpack .../26-ruby-test-unit_3.2.5-1_all.deb ...\n",
            "Unpacking ruby-test-unit (3.2.5-1) ...\n",
            "Selecting previously unselected package libruby2.5:amd64.\n",
            "Preparing to unpack .../27-libruby2.5_2.5.1-1ubuntu1.12_amd64.deb ...\n",
            "Unpacking libruby2.5:amd64 (2.5.1-1ubuntu1.12) ...\n",
            "Selecting previously unselected package libsynctex1:amd64.\n",
            "Preparing to unpack .../28-libsynctex1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexlua52:amd64.\n",
            "Preparing to unpack .../29-libtexlua52_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexluajit2:amd64.\n",
            "Preparing to unpack .../30-libtexluajit2_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libzzip-0-13:amd64.\n",
            "Preparing to unpack .../31-libzzip-0-13_0.13.62-3.1ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package lmodern.\n",
            "Preparing to unpack .../32-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package preview-latex-style.\n",
            "Preparing to unpack .../33-preview-latex-style_11.91-1ubuntu1_all.deb ...\n",
            "Unpacking preview-latex-style (11.91-1ubuntu1) ...\n",
            "Selecting previously unselected package t1utils.\n",
            "Preparing to unpack .../34-t1utils_1.41-2_amd64.deb ...\n",
            "Unpacking t1utils (1.41-2) ...\n",
            "Selecting previously unselected package tex-gyre.\n",
            "Preparing to unpack .../35-tex-gyre_20160520-1_all.deb ...\n",
            "Unpacking tex-gyre (20160520-1) ...\n",
            "Selecting previously unselected package texlive-binaries.\n",
            "Preparing to unpack .../36-texlive-binaries_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package texlive-base.\n",
            "Preparing to unpack .../37-texlive-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-fonts-recommended.\n",
            "Preparing to unpack .../38-texlive-fonts-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-plain-generic.\n",
            "Preparing to unpack .../39-texlive-plain-generic_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-plain-generic (2017.20180305-2) ...\n",
            "Selecting previously unselected package texlive-generic-recommended.\n",
            "Preparing to unpack .../40-texlive-generic-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-generic-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-base.\n",
            "Preparing to unpack .../41-texlive-latex-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-recommended.\n",
            "Preparing to unpack .../42-texlive-latex-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-pictures.\n",
            "Preparing to unpack .../43-texlive-pictures_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-pictures (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-extra.\n",
            "Preparing to unpack .../44-texlive-latex-extra_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-latex-extra (2017.20180305-2) ...\n",
            "Selecting previously unselected package tipa.\n",
            "Preparing to unpack .../45-tipa_2%3a1.3-20_all.deb ...\n",
            "Unpacking tipa (2:1.3-20) ...\n",
            "Selecting previously unselected package texlive-xetex.\n",
            "Preparing to unpack .../46-texlive-xetex_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-xetex (2017.20180305-1) ...\n",
            "Setting up libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.17) ...\n",
            "Setting up libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libjs-jquery (3.2.1-1) ...\n",
            "Setting up libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Setting up libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up tex-common (6.09) ...\n",
            "update-language: texlive-base not installed and configured, doing nothing!\n",
            "Setting up poppler-data (0.4.8-2) ...\n",
            "Setting up tex-gyre (20160520-1) ...\n",
            "Setting up preview-latex-style (11.91-1ubuntu1) ...\n",
            "Setting up fonts-texgyre (20160520-1) ...\n",
            "Setting up fonts-noto-mono (20171026-2) ...\n",
            "Setting up fonts-lato (2.0-2) ...\n",
            "Setting up libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Setting up libcupsimage2:amd64 (2.2.7-1ubuntu2.9) ...\n",
            "Setting up libjbig2dec0:amd64 (0.13-6) ...\n",
            "Setting up ruby-did-you-mean (1.2.0-2) ...\n",
            "Setting up t1utils (1.41-2) ...\n",
            "Setting up ruby-net-telnet (0.1.1-2) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-13) ...\n",
            "Setting up rubygems-integration (1.11) ...\n",
            "Setting up libpotrace0 (1.14-2) ...\n",
            "Setting up javascript-common (11) ...\n",
            "Setting up ruby-minitest (5.10.3-1) ...\n",
            "Setting up libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Setting up libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.17) ...\n",
            "Setting up libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-lmodern (2.004.5-3) ...\n",
            "Setting up ruby-power-assert (0.3.0-1) ...\n",
            "Setting up texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n",
            "update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n",
            "Setting up texlive-base (2017.20180305-1) ...\n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R... \n",
            "mktexlsr: Done.\n",
            "tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n",
            "tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n",
            "tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n",
            "tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/config/pdftexconfig.tex\n",
            "Setting up texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-plain-generic (2017.20180305-2) ...\n",
            "Setting up texlive-generic-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-latex-base (2017.20180305-1) ...\n",
            "Setting up lmodern (2.004.5-3) ...\n",
            "Setting up texlive-latex-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-pictures (2017.20180305-1) ...\n",
            "Setting up tipa (2:1.3-20) ...\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-DEBIAN'... done.\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST'... done.\n",
            "update-fmtutil has updated the following file(s):\n",
            "\t/var/lib/texmf/fmtutil.cnf-DEBIAN\n",
            "\t/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST\n",
            "If you want to activate the changes in the above file(s),\n",
            "you should run fmtutil-sys or fmtutil.\n",
            "Setting up texlive-latex-extra (2017.20180305-2) ...\n",
            "Setting up texlive-xetex (2017.20180305-1) ...\n",
            "Setting up ruby2.5 (2.5.1-1ubuntu1.12) ...\n",
            "Setting up ruby (1:2.5.1) ...\n",
            "Setting up ruby-test-unit (3.2.5-1) ...\n",
            "Setting up rake (12.3.1-1ubuntu0.1) ...\n",
            "Setting up libruby2.5:amd64 (2.5.1-1ubuntu1.12) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for tex-common (6.09) ...\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Building format(s) --all.\n",
            "\tThis may take some time... done.\n",
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/Colab Notebooks/EECS442-504_PS4_FA_2022.ipynb to PDF\n",
            "[NbConvertApp] Support files will be in EECS442-504_PS4_FA_2022_files/\n",
            "[NbConvertApp] Making directory ./EECS442-504_PS4_FA_2022_files\n",
            "[NbConvertApp] Making directory ./EECS442-504_PS4_FA_2022_files\n",
            "[NbConvertApp] Writing 159850 bytes to ./notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 139512 bytes to /content/drive/MyDrive/Colab Notebooks/EECS442-504_PS4_FA_2022.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_56657194-82b3-49f2-9483-01ca8b23cff4\", \"EECS442-504_PS4_FA_2022.pdf\", 139512)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# generate pdf\n",
        "# Please provide the full path of the notebook file below\n",
        "# Important: make sure that your file name does not contain spaces!\n",
        "\n",
        "# Ex: notebookpath = '/content/drive/My Drive/Colab Notebooks/EECS_442_PS4_FA_2022_Starter_Code.ipynb'\n",
        "notebookpath = '/content/drive/MyDrive/Colab Notebooks/EECS442-504_PS4_FA_2022.ipynb' \n",
        "\n",
        "file_name = notebookpath.split('/')[-1]\n",
        "get_ipython().system(\"apt update && apt install texlive-xetex texlive-fonts-recommended texlive-generic-recommended\")\n",
        "get_ipython().system(\"jupyter nbconvert --to PDF {}\".format(notebookpath.replace(' ', '\\\\ ')))\n",
        "files.download(notebookpath.split('.')[0]+'.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dNfMOpFZtfnq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Fy-7c_kJKUCd",
        "CdquTNqGKYcc"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}