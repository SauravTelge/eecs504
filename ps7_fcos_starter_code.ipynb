{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SauravTelge/eecs504/blob/main/ps7_fcos_starter_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EECS 442/504 PS7: Object Detection\n",
        "\n",
        "__Please provide the following information__\n",
        "(e.g. Andrew Owens, ahowens):\n",
        "\n",
        "Saurav Telge, sauravt\n",
        "\n",
        "__Important__: after you download the .ipynb file, please name it as __\"PS\\<this_ps_number\\>_\\<your_uniqname\\>.ipynb\"__ before you submit it to canvas. Example: adam_01101100.ipynb.\n",
        "\n"
      ],
      "metadata": {
        "id": "a_7QO_fWWYS-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SWLXqQWEIEZ"
      },
      "source": [
        "# FCOS: A Simple One-Stage and Anchor-Free Object Detector\n",
        "\n",
        "In this exercise you will implement a single-stage object detector based on [FCOS: Fully-Convolutional One-Stage Object Detection](https://arxiv.org/abs/1904.01355) and train it to detect a set of object classes.\n",
        "Our detector design is highly similar to FCOS itself, except we train a smaller model with slightly different hyperparameters to manage with limited resources on Colab.\n",
        "\n",
        "This problem set will use a small version of the popular image detection dataset Pascal VOC. \n",
        "\n",
        "We will also evaluate the detection accuracy using the mean Average Precision ([mAP](https://github.com/Cartucho/mAP))."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Acknowledgement**: This problem set has been adapted from EECS 598/498 Deep Learning for Computer Vision taught by Prof. Justin Johnson."
      ],
      "metadata": {
        "id": "-gLAyJuEi6t9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCsHXOHhFF_2"
      },
      "source": [
        "# 7.1 (a) Getting started "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjsv4DYuz5Av"
      },
      "outputs": [],
      "source": [
        "#install some python modules we need\n",
        "!pip -q install wget\n",
        "!pip -q install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYC9wnhjzuMR"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data._utils.collate import default_collate\n",
        "from torch import optim\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from torchvision.models import feature_extraction\n",
        "from torchvision.ops import sigmoid_focal_loss,nms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "from typing import Optional, Dict, List, Optional, Tuple\n",
        "import wget\n",
        "import gdown\n",
        "import json\n",
        "import random\n",
        "import tarfile\n",
        "import time\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWw_ZC8h0IXM"
      },
      "outputs": [],
      "source": [
        "# check GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using the GPU!\")\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    print(\"WARNING: Could not find GPU! Using CPU only. If you want to enable GPU, please to go Edit > Notebook Settings > Hardware Accelerator and select GPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP6m6b4fdee3"
      },
      "outputs": [],
      "source": [
        "# for plotting\n",
        "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
        "plt.rcParams[\"font.size\"] = 16\n",
        "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
        "plt.rcParams[\"image.cmap\"] = \"gray\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4mWm08CdjI-"
      },
      "outputs": [],
      "source": [
        "# some global variables, do not overwrite later\n",
        "NUM_CLASSES = 20\n",
        "BATCH_SIZE = 16\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "NUM_WORKERS = 2\n",
        "TensorDict = Dict[str, torch.Tensor]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBN9YsqGF0cv"
      },
      "source": [
        "## Import helper functions \n",
        "\n",
        "In this problem, we have written the dataloader for you. This dataloader and some other helper functions have been consolidated in a python file which will be downloaded and imported in the follwing cells. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-nvCeg7H0uQ"
      },
      "outputs": [],
      "source": [
        "# download the helpers file \n",
        "gdown.download(url = \"https://drive.google.com/uc?export=download&id=1dXe4i306p99plNMkUBjbsvfcSIYTy6hM\", output=\"/content/ps7_helpers.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "***Please delete this cell before pdf conversion***\n",
        "```\n",
        "\n",
        "You can take a look at the file if you wish to know more about the helper code. This file is named `ps7_helpers.py` and you can find it in the instance storage (as shown) once it is downloaded. You can also double click on it to open view and edit it in this Colab environment itself.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1YUIRDnwEYO6i9N8sTzIhlm2_FXshGkyn)"
      ],
      "metadata": {
        "id": "tcMd7mEscpZr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-AoaoqvHeQN"
      },
      "outputs": [],
      "source": [
        "from ps7_helpers import VOC2007Detection, train_detector, infinite_loader, reset_seed, detection_visualizer, rel_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa3B4dUHH5bE"
      },
      "source": [
        "## Load dataset and visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llh6TuWQIMSI"
      },
      "source": [
        "To train our detector, we need to convert individual images (JPEG) and annotations (XML files) into batches of tensors. We perform this by wrapping our datasets with a PyTorch [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) object.\n",
        "We have implemented it in `ps7_helpers.py` - you are not required to understand its implementation, however you should understand the format of its output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ON8YcnEYdrO_"
      },
      "outputs": [],
      "source": [
        "train_dataset = VOC2007Detection(\n",
        "    \"/content\", \"train\", image_size=IMAGE_SHAPE[0],\n",
        "    download=True  # True (for the first time and every subsequent reconnect of runtime)\n",
        ")\n",
        "val_dataset = VOC2007Detection(\"/content\", \"val\", image_size=IMAGE_SHAPE[0])\n",
        "\n",
        "print(\"Dataset sizes: train ({}), val ({})\".format(len(train_dataset),len(val_dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RNGRfx1dtg-"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=NUM_WORKERS\n",
        ")\n",
        "  \n",
        "# Use batch_size = 1 during inference - during inference we do not center crop\n",
        "# the image to detect all objects, hence they may be of different size. It is\n",
        "# easier and less redundant to use batch_size=1 rather than zero-padding images.\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=1, pin_memory=True, num_workers=NUM_WORKERS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2ufaTiZIbFf"
      },
      "source": [
        "The `DataLoader` objects return batches of data.\n",
        "\n",
        "The first output from the `DataLoader` is a Tensor `image` of shape `(B, 3, IMAGE_SHAPE[0], IMAGE_SHAPE[1])`. This is a batch of `B` images, similar to what we have seen in classification datasets.\n",
        "\n",
        "The second output from the `DataLoader` is a Tensor `gt_boxes` of shape `(B, N, 5)` giving information about all objects in all images of the batch. `gt_boxes[i, j] = (x1, y1, x2, y2, C)` gives information about the `j`th object in `image[i]`. The position of the top-left corner of the box is `(x1, y1)` and the position of the bottom-right corner of the box is `(x2, x2)`. These coordinates are real-valued in `[0, 224]`. `C` is an integer giving the category label for this bounding box. This `(x1, y1, x2, y2)` format for bounding boxes is commonly referred as XYXY format.\n",
        "\n",
        "Each image can have different numbers of objects. If `image[i]` has $N_i$ objects, then $N = \\max_i(N_i)$ is the maximum number of objects per image among all objects in the batch; this value can vary from batch to batch. For the images that have fewer than $N$ annotated objects, only the first $N_i$ rows of `gt_boxes[i]` contain annotations; the remaining rows are padded with -1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZRtVynnIfFo"
      },
      "outputs": [],
      "source": [
        "train_loader_iter = iter(train_loader)\n",
        "image_paths, images, gt_boxes = train_loader_iter.next()\n",
        "\n",
        "print(\"image paths           : {}\".format(image_paths))\n",
        "print(\"image batch has shape : {}\".format(images.shape))\n",
        "print(\"gt_boxes has shape    : {}\".format(gt_boxes.shape))\n",
        "\n",
        "print(\"Five boxes per image  :\")\n",
        "print(gt_boxes[:, 0, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G3woAg2IjTA"
      },
      "source": [
        "## Visualize PASCAL VOC 2007\n",
        "\n",
        "Before starting to build your model, it is highly recommended that you visualize your training data and observe some examples. This can help uncover any bugs in dataloading and sometimes even give you strong intuitions to include a modeling component!\n",
        "\n",
        "We also use a function to visualize our detections, implemented in `ps7_helpers`. You are not required to understand it but we encourage you to read it! Here we sample some images from the PASCAL VOC 2007 training set, and visualize the ground-truth object boxes and category labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvuv8qm4IkGE"
      },
      "outputs": [],
      "source": [
        "# Define an \"inverse\" transform for the image that un-normalizes by ImageNet \n",
        "# color and mean. Without this, the images will NOT be visually understandable. \n",
        "# You can check how it will look otherwise by just commenting line which applies\n",
        "# the inverser norm.\n",
        "\n",
        "inverse_norm = transforms.Compose(\n",
        "    [\n",
        "        transforms.Normalize(mean=[0., 0., 0.], std=[1 / 0.229, 1 / 0.224, 1 / 0.225]),\n",
        "        transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1., 1., 1.]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "for idx, (_, image, gt_boxes) in enumerate(train_dataset):\n",
        "    # Stop after visualizing three images.\n",
        "    if idx > 2:\n",
        "        break\n",
        "\n",
        "    # Un-normalize image to bring in [0, 1] RGB range.\n",
        "    image = inverse_norm(image)\n",
        "\n",
        "    # Remove padded boxes from visualization.\n",
        "    is_valid = gt_boxes[:, 4] >= 0\n",
        "    detection_visualizer(image, val_dataset.idx_to_class, gt_boxes[is_valid])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "***Please delete this cell before pdf conversion***\n",
        "```\n",
        "# Implementing FCOS\n",
        "\n",
        "\n",
        "FCOS is a fully-convolutional one-stage object detection model — unlike two-stage detectors like Faster R-CNN, it does not comprise any custom modules like anchor boxes, RoI pooling/align, and RPN proposals.\n",
        "\n",
        "An overview of the model in shown below. In case it does not load, see [Figure 2 in FCOS paper](https://arxiv.org/abs/1904.01355).\n",
        "It details three modeling components: backbone, feature pyramid network (FPN), and head (prediction layers).\n",
        "First, we will implement FCOS as shown in this figure, and then implement components to train it with the PASCAL VOC 2007 dataset we loaded above.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1jES3NIyIcgEwzAn3zqF2F1-NL5yYLYcF \"Modified FCOS\")\n",
        "\n",
        "> **CAUTION:** The original FCOS model (as per original paper and lecture slides) places the centerness predictor in parallel with classification predictor. However, we will follow the widely prevalent implementation practice to place the centerness predictor in parallel with box regression predictor (as shown in figure above).\n",
        "The main intuition is that centerness and box regression are localization-related quantities and hence would benefit to have shared features."
      ],
      "metadata": {
        "id": "cd3J-hvocuYf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tsOvXUeJW-y"
      },
      "source": [
        "## 7.1 (b) Implementing Backbone and Feature Pyramid Network \n",
        "\n",
        "First, we start building the backbone and FPN of our detector (blue and green parts above). It is the core component that takes in an image and outputs its features of different scales. It can be any type of convolutional network that progressively downsamples the image.\n",
        "\n",
        "Here, we use a small [RegNetX-400MF](https://pytorch.org/vision/stable/models.html#torchvision.models.regnet_x_400mf) as the backbone so we can train in reasonable time on Colab. We have already implemented the minimal logic to initialize this backbone from pre-trained ImageNet weights and extract intermediate features `(c3, c4, c5)` as shown in the figure above.\n",
        "These features `(c3, c4, c5)` have height and width that is ${1/8}^{th}$, ${1/16}^{th}$, and ${1/32}^{th}$ of the input image respectively.\n",
        "These values `(8, 16, 32)` are called the \"stride\" of these features.\n",
        "In other words, it means that moving one location on the FPN level is equivalent to moving `stride` pixels in the input image.\n",
        "\n",
        "You need to implement extra modules to attach the FPN to this backbone. For more details, see Figure 3 in [FPN paper](https://arxiv.org/abs/1612.03144).\n",
        "FPN will convert these `(c3, c4, c5)` multi-scale features to `(p3, p4, p5)`. These notations \"p3\", \"p4\", \"p5\" are called _FPN levels_.\n",
        "\n",
        "Before you write any code, let's initialize the backbone in the next cell. You should see the shape of `(c3, c4, c5)` features for an input image:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDC4pAH4Jkm7"
      },
      "source": [
        "Follow the instructions below to implement additional FPN layers for transforming `(c3, c4, c5)` to `(p3, p4, p5)`.\n",
        "For training a small enough model on Google Colab, we leave out `(p6, p7)` as shown in the Figure.\n",
        "Output features from these FPN levels are expected to have same height and width as backbone features, but now they should have the same number of channels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Enjw0c-hfqg2"
      },
      "outputs": [],
      "source": [
        "class DetectorBackboneWithFPN(nn.Module):\n",
        "    r\"\"\"\n",
        "    Detection backbone network: A tiny RegNet model coupled with a Feature\n",
        "    Pyramid Network (FPN). This model takes in batches of input images with\n",
        "    shape `(B, 3, H, W)` and gives features from three different FPN levels\n",
        "    with shapes and total strides upto that level:\n",
        "\n",
        "        - level p3: (out_channels, H /  8, W /  8)      stride =  8\n",
        "        - level p4: (out_channels, H / 16, W / 16)      stride = 16\n",
        "        - level p5: (out_channels, H / 32, W / 32)      stride = 32\n",
        "\n",
        "    NOTE: We could use any convolutional network architecture that progressively\n",
        "    downsamples the input image and couple it with FPN. We use a small enough\n",
        "    backbone that can work with Colab GPU and get decent enough performance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        # Initialize with ImageNet pre-trained weights.\n",
        "        _cnn = models.regnet_x_400mf(weights=\"IMAGENET1K_V2\")\n",
        "\n",
        "        # Torchvision models only return features from the last level. Detector\n",
        "        # backbones (with FPN) require intermediate features of different scales.\n",
        "        # So we wrap the ConvNet with torchvision's feature extractor. Here we\n",
        "        # will get output features with names (c3, c4, c5) with same stride as\n",
        "        # (p3, p4, p5) described above.\n",
        "        self.backbone = feature_extraction.create_feature_extractor(\n",
        "            _cnn,\n",
        "            return_nodes={\n",
        "                \"trunk_output.block2\": \"c3\",\n",
        "                \"trunk_output.block3\": \"c4\",\n",
        "                \"trunk_output.block4\": \"c5\",\n",
        "            },\n",
        "        )\n",
        "\n",
        "        # Pass a dummy batch of input images to infer shapes of (c3, c4, c5).\n",
        "        # Features are a dictionary with keys as defined above. Values are\n",
        "        # batches of tensors in NCHW format, that give intermediate features\n",
        "        # from the backbone network.\n",
        "        dummy_out = self.backbone(torch.randn(2, 3, 224, 224))\n",
        "        dummy_out_shapes = [(key, value.shape) for key, value in dummy_out.items()]\n",
        "\n",
        "        print(\"For dummy input images with shape: (2, 3, 224, 224)\")\n",
        "        for level_name, feature_shape in dummy_out_shapes:\n",
        "            print(f\"Shape of {level_name} features: {feature_shape}\")\n",
        "\n",
        "        ######################################################################\n",
        "        # TODO: Initialize additional Conv layers for FPN.                   #\n",
        "        #                                                                    #\n",
        "        # Create THREE \"lateral\" 1x1 conv layers to transform (c3, c4, c5)   #\n",
        "        # such that they all end up with the same `out_channels`.            #\n",
        "        # Then create THREE \"output\" 3x3 conv layers to transform the merged #\n",
        "        # FPN features to output (p3, p4, p5) features.                      #\n",
        "        # All conv layers must have stride=1 and padding such that features  #\n",
        "        # do not get downsampled due to 3x3 convs.                           #\n",
        "        #                                                                    #\n",
        "        # HINT: You have to use `dummy_out_shapes` defined above to decide   #\n",
        "        # the input/output channels of these layers.                         #\n",
        "        ######################################################################\n",
        "        # This behaves like a Python dict, but makes PyTorch understand that\n",
        "        # there are trainable weights inside it.\n",
        "        # Add THREE lateral 1x1 conv and THREE output 3x3 conv layers.\n",
        "        self.fpn_params = nn.ModuleDict()\n",
        "\n",
        "        # Replace \"pass\" statement with your code\n",
        "\n",
        "      self.conv1=nn.conv2d(1,64,1,1)\n",
        "      self.conv2=nn.conv2d()\n",
        "\n",
        "        \n",
        "\n",
        "        ######################################################################\n",
        "        #                            END OF YOUR CODE                        #\n",
        "        ######################################################################\n",
        "\n",
        "    @property\n",
        "    def fpn_strides(self):\n",
        "        \"\"\"\n",
        "        Total stride up to the FPN level. For a fixed ConvNet, these values\n",
        "        are invariant to input image size. You may access these values freely\n",
        "        to implement your logic in FCOS / Faster R-CNN.\n",
        "        \"\"\"\n",
        "        return {\"p3\": 8, \"p4\": 16, \"p5\": 32}\n",
        "\n",
        "    def forward(self, images: torch.Tensor):\n",
        "\n",
        "        # Multi-scale features, dictionary with keys: {\"c3\", \"c4\", \"c5\"}.\n",
        "        backbone_feats = self.backbone(images)\n",
        "\n",
        "        fpn_feats = {\"p3\": None, \"p4\": None, \"p5\": None}\n",
        "        ######################################################################\n",
        "        # TODO: Fill output FPN features (p3, p4, p5) using RegNet features  #\n",
        "        # (c3, c4, c5) and FPN conv layers created above.                    #\n",
        "        # HINT: Use `F.interpolate` to upsample FPN features.                #\n",
        "        ######################################################################\n",
        "\n",
        "        # Replace \"pass\" statement with your code\n",
        "\n",
        "        \n",
        "        \n",
        "        ######################################################################\n",
        "        #                            END OF YOUR CODE                        #\n",
        "        ######################################################################\n",
        "\n",
        "        return fpn_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlFzBaySf0S1"
      },
      "outputs": [],
      "source": [
        "backbone = DetectorBackboneWithFPN(out_channels=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyCYKTXEJuFa"
      },
      "source": [
        "## 7.1 (c) Implementing FCOS prediction network (head)\n",
        "\n",
        "By this point, you have implemented the backbone and FPN.  \n",
        "\n",
        "Now we implement the \"head\" prediction layers (**orange** blocks in the Figure above). This head has shared weights across all FPN levels, and its purpose is to predict an object class, bounding box, and centerness at every location.\n",
        "\n",
        "Look closely at the right block in the Figure which shows the inner components of head. It comprises 4-convolution layers that produce `(H, W, 256)` features (different H, W for different FPN levels) and then use one convolutional layer each to make final predictions. \n",
        "\n",
        "You will now add these modules in `FCOSPredictionNetwork` - follow instructions in its documentation.\n",
        "Below we show how to initialize and use this module.\n",
        "\n",
        "In the expected output the classification logits have `NUM_CLASSES` channels, box regression deltas have 4 output channels, and centerness has 1 output channels.\n",
        "The height and width of all outputs is flattened to one dimension, resulting in `(B, H * W, C)` format - this format is more convenient for computing loss, as you will see later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-e8wt5rpf-AJ"
      },
      "outputs": [],
      "source": [
        "class FCOSPredictionNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    FCOS prediction network that accepts FPN feature maps from different levels\n",
        "    and makes three predictions at every location: bounding boxes, class ID and\n",
        "    centerness. This module contains a \"stem\" of convolution layers, along with\n",
        "    one final layer per prediction. For a visual depiction, see Figure 2 (right\n",
        "    side) in FCOS paper: https://arxiv.org/abs/1904.01355\n",
        "\n",
        "    We will use feature maps from FPN levels (P3, P4, P5) and exclude (P6, P7).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, num_classes: int, in_channels: int, stem_channels: List[int]\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_classes: Number of object classes for classification.\n",
        "            in_channels: Number of channels in input feature maps. This value\n",
        "                is same as the output channels of FPN, since the head directly\n",
        "                operates on them.\n",
        "            stem_channels: List of integers giving the number of output channels\n",
        "                in each convolution layer of stem layers (orange blocks in the Figure above).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        ######################################################################\n",
        "        # TODO: Create a stem of alternating 3x3 convolution layers and RELU\n",
        "        # activation modules. Note there are two separate stems for class and\n",
        "        # box stem. The prediction layers for box regression and centerness\n",
        "        # operate on the output of `stem_box`.\n",
        "        # See FCOS figure again; both stems are identical.\n",
        "        #\n",
        "        # Use `in_channels` and `stem_channels` for creating these layers, the\n",
        "        # docstring above tells you what they mean. Initialize weights of each\n",
        "        # conv layer from a normal distribution with mean = 0 and std dev = 0.01\n",
        "        # and all biases with zero. Use conv stride = 1 and zero padding such\n",
        "        # that size of input features remains same: remember we need predictions\n",
        "        # at every location in feature map, we shouldn't \"lose\" any locations.\n",
        "        ######################################################################\n",
        "        # Fill these.\n",
        "        stem_cls = []\n",
        "        stem_box = []\n",
        "\n",
        "        # Replace \"pass\" statement with your code\n",
        "\n",
        "        pass\n",
        "\n",
        "        ######################################################################\n",
        "        \n",
        "        # Wrap the layers defined by student into a `nn.Sequential` module:\n",
        "        self.stem_cls = nn.Sequential(*stem_cls)\n",
        "        self.stem_box = nn.Sequential(*stem_box)\n",
        "\n",
        "        ######################################################################\n",
        "        # TODO: Create THREE 3x3 conv layers for individually predicting three\n",
        "        # things at every location of feature map:\n",
        "        #     1. object class logits (`num_classes` outputs)\n",
        "        #     2. box regression deltas (4 outputs: LTRB deltas from locations)\n",
        "        #     3. centerness logits (1 output)\n",
        "        #\n",
        "        # Class probability and actual centerness are obtained by applying\n",
        "        # sigmoid activation to these logits. However, DO NOT initialize those\n",
        "        # modules here. This module should always output logits; PyTorch loss\n",
        "        # functions have numerically stable implementations with logits. During\n",
        "        # inference, logits are converted to probabilities by applying sigmoid,\n",
        "        # BUT OUTSIDE this module.\n",
        "        #\n",
        "        ######################################################################\n",
        "\n",
        "        # Replace these lines with your code, keep variable names unchanged.\n",
        "        self.pred_cls = None  # Class prediction conv\n",
        "        self.pred_box = None  # Box regression conv\n",
        "        self.pred_ctr = None  # Centerness conv\n",
        "\n",
        "        # Replace \"pass\" statement with your code\n",
        "\n",
        "        pass\n",
        "\n",
        "        ######################################################################\n",
        "        #                           END OF YOUR CODE                         #\n",
        "        ######################################################################\n",
        "\n",
        "        # OVERRIDE: Use a negative bias in `pred_cls` to improve training\n",
        "        # stability. Without this, the training will most likely diverge.\n",
        "        # STUDENTS: You do not need to get into details of why this is needed.\n",
        "        torch.nn.init.constant_(self.pred_cls.bias, -math.log(99))\n",
        "\n",
        "    def forward(self, feats_per_fpn_level: TensorDict) -> List[TensorDict]:\n",
        "        \"\"\"\n",
        "        Accept FPN feature maps and predict the desired outputs at every location\n",
        "        (as described above). Format them such that channels are placed at the\n",
        "        last dimension, and (H, W) are flattened (having channels at last is\n",
        "        convenient for computing loss as well as perforning inference).\n",
        "\n",
        "        Args:\n",
        "            feats_per_fpn_level: Features from FPN, keys {\"p3\", \"p4\", \"p5\"}. Each\n",
        "                tensor will have shape `(batch_size, fpn_channels, H, W)`. For an\n",
        "                input (224, 224) image, H = W are (28, 14, 7) for (p3, p4, p5).\n",
        "\n",
        "        Returns:\n",
        "            List of dictionaries, each having keys {\"p3\", \"p4\", \"p5\"}:\n",
        "            1. Classification logits: `(batch_size, H * W, num_classes)`.\n",
        "            2. Box regression deltas: `(batch_size, H * W, 4)`\n",
        "            3. Centerness logits:     `(batch_size, H * W, 1)`\n",
        "        \"\"\"\n",
        "\n",
        "        ######################################################################\n",
        "        # TODO: Iterate over every FPN feature map and obtain predictions using\n",
        "        # the layers defined above. Remember that prediction layers of box\n",
        "        # regression and centerness will operate on output of `stem_box`,\n",
        "        # and classification layer operates separately on `stem_cls`.\n",
        "        #\n",
        "        # CAUTION: The original FCOS model uses shared stem for centerness and\n",
        "        # classification. Recent follow-up papers commonly place centerness and\n",
        "        # box regression predictors with a shared stem, which we follow here.\n",
        "        #\n",
        "        # DO NOT apply sigmoid to classification and centerness logits.\n",
        "        ######################################################################\n",
        "        # Fill these with keys: {\"p3\", \"p4\", \"p5\"}, same as input dictionary.\n",
        "        class_logits = {}\n",
        "        boxreg_deltas = {}\n",
        "        centerness_logits = {}\n",
        "\n",
        "        # Replace \"pass\" statement with your code\n",
        "\n",
        "        pass\n",
        "            \n",
        "        ######################################################################\n",
        "        #                           END OF YOUR CODE                         #\n",
        "        ######################################################################\n",
        "\n",
        "        return [class_logits, boxreg_deltas, centerness_logits]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the cell below should give output similar to this:\n",
        "\n",
        "```\n",
        "FCOS prediction network parameters:\n",
        "FCOSPredictionNetwork(\n",
        "  (stem_cls): Sequential(\n",
        "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (1): ReLU()\n",
        "    (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (3): ReLU()\n",
        "  )\n",
        "  (stem_box): Sequential(\n",
        "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (1): ReLU()\n",
        "    (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (3): ReLU()\n",
        "  )\n",
        "  (pred_cls): Conv2d(64, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  (pred_box): Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  (pred_ctr): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        ")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "q-82mC9DiVnN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcIPf9RbgDrl"
      },
      "outputs": [],
      "source": [
        "# Tiny head with `in_channels` as FPN output channels in prior cell,\n",
        "# and two conv layers in stem.\n",
        "pred_net = FCOSPredictionNetwork(\n",
        "    num_classes=NUM_CLASSES, in_channels=64, stem_channels=[128, 64]\n",
        ")\n",
        "\n",
        "print(\"FCOS prediction network parameters:\")\n",
        "print(pred_net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlNAwVcGJ8qZ"
      },
      "source": [
        "## Train the FCOS\n",
        "\n",
        "You have now finished implementing FCOS and it is ready to train!\n",
        "Did you notice that all the trainable layers you implemented so far — in FPN and in prediction layers — are convolutional layers?\n",
        "Due to this design, FCOS gets _fully-convolutional_ in its name!\n",
        "It performs object detection with _only_ convolution layers, and no special anchors, proposals etc.\n",
        "The next few steps focus on preparing the training pipeline for FCOS.\n",
        "\n",
        "Recall that image classification models are trained with `(image, label)` pairs,\n",
        "where `label` is an integer (more concretely a one-hot vector) that is associated with the _entire_ image.\n",
        "You cannot train an object detector in this fashion because of two reasons:\n",
        "\n",
        "1. Each image has a variable number of bounding boxes (and their class labels).\n",
        "2. Any class label is not associated with an entire image, but rather only a small region enclosed by the bounding box.\n",
        "\n",
        "Since the object detector classifies every location in feature map along with a bounding box and centerness, we need to supervise every prediction with _one and only one_ GT target. In next few cells, you will work through this assignment procedure between predictions and GT boxes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9zdrOAUKESg"
      },
      "source": [
        "### 7.1 (d) Assigning a GT target to every model prediction\n",
        "FCOS heads make three predictions at every location: object class, bounding box, and centerness. We need to assign a GT target for each of them during training. All three predictions correspond to a single location on an FPN level `(p3, p4, p5)`, so instead we can view this problem as assigning GT boxes (and their class labels) to every FPN feature map location.\n",
        "\n",
        "GT boxes are available (from the dataloader) as 5D vectors `(x1, y1, x2, y2, C)` where `(x1, y1)` is the top-left co-ordinate and `(x2, y2)` is the bottom-right co-ordinate of the bounding box, and C is its object class label. These co-ordinates are absolute and real-valued in image dimensions. To begin with the assignment, we will represent every location on an FPN level with (xc, yc) absolute and real-valued co-ordinates of a point on the image, that are centers of the receptive fields of those features.\n",
        "\n",
        "For example, given features from FPN level having shape `(batch_size, channels, H / stride, W / stride)` and the location `feature[:, :, i, j]` will map to the image pixel `(stride * (i + 0.5), stride * (j + 0.5)) - 0.5` indicates the shift from top-left corner to the center of \"stride box\".\n",
        "\n",
        "Implement the `get_fpn_location_coords` to get (xc, yc) location co-ordinates of all FPN features. Follow its documentation and see its usage example in the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhRE2k8OgKxY"
      },
      "outputs": [],
      "source": [
        "def get_fpn_location_coords(\n",
        "    shape_per_fpn_level: Dict[str, Tuple],\n",
        "    strides_per_fpn_level: Dict[str, int],\n",
        "    dtype: torch.dtype = torch.float32,\n",
        "    device: str = \"cpu\",\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Map every location in FPN feature map to a point on the image. This point\n",
        "    represents the center of the receptive field of this location. We need to\n",
        "    do this for having a uniform co-ordinate representation of all the locations\n",
        "    across FPN levels, and GT boxes.\n",
        "\n",
        "    Args:\n",
        "        shape_per_fpn_level: Shape of the FPN feature level, dictionary of keys\n",
        "            {\"p3\", \"p4\", \"p5\"} and feature shapes `(B, C, H, W)` as values.\n",
        "        strides_per_fpn_level: Dictionary of same keys as above, each with an\n",
        "            integer value giving the stride of corresponding FPN level.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, torch.Tensor]\n",
        "            Dictionary with same keys as `shape_per_fpn_level` and values as\n",
        "            tensors of shape `(H * W, 2)` giving `(xc, yc)` co-ordinates of the\n",
        "            centers of receptive fields of the FPN locations, on input image.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set these to `(N, 2)` Tensors giving absolute location co-ordinates.\n",
        "    location_coords = {\n",
        "        level_name: None for level_name, _ in shape_per_fpn_level.items()\n",
        "    }\n",
        "\n",
        "    for level_name, feat_shape in shape_per_fpn_level.items():\n",
        "        level_stride = strides_per_fpn_level[level_name]\n",
        "\n",
        "        ######################################################################\n",
        "        # TODO: Implement logic to get location co-ordinates below.          #\n",
        "        ######################################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "\n",
        "        pass\n",
        "        \n",
        "        ######################################################################\n",
        "        #                             END OF YOUR CODE                       #\n",
        "        ######################################################################\n",
        "    return location_coords\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Running the cell below should give output similar to this:\n",
        "\n",
        "```\n",
        "Extra FPN modules added:\n",
        "ModuleDict(\n",
        "  (conv5): Conv2d(400, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "  (conv4): Conv2d(160, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "  (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "  (conv3_out): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  (conv4_out): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  (conv5_out): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        ")\n",
        "For dummy input images with shape: torch.Size([2, 3, 224, 224])\n",
        "Shape of p3 features: torch.Size([2, 64, 28, 28])\n",
        "Shape of p4 features: torch.Size([2, 64, 14, 14])\n",
        "Shape of p5 features: torch.Size([2, 64, 7, 7])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ipLiWSR2liHq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d11Q0x_5Wlyi"
      },
      "outputs": [],
      "source": [
        "print(\"Extra FPN modules added:\")\n",
        "print(backbone.fpn_params)\n",
        "\n",
        "# Pass a batch of dummy images (random tensors) in NCHW format and observe the output.\n",
        "dummy_images = torch.randn(2, 3, 224, 224)\n",
        "\n",
        "# Collect dummy output.\n",
        "dummy_fpn_feats = backbone(dummy_images)\n",
        "\n",
        "print(f\"For dummy input images with shape: {dummy_images.shape}\")\n",
        "for level_name, feat in dummy_fpn_feats.items():\n",
        "    print(f\"Shape of {level_name} features: {feat.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkcW8B4OKJhv"
      },
      "outputs": [],
      "source": [
        "# Get shapes of each FPN level feature map. We don't call these \"dummy\" because\n",
        "# they don't depend on the _values_ of features, but rather only shapes.\n",
        "fpn_feats_shapes = {\n",
        "    level_name: feat.shape for level_name, feat in dummy_fpn_feats.items()\n",
        "}\n",
        "\n",
        "# Get CPU tensors for this sanity check: (you can pass `device=` argument.\n",
        "locations_per_fpn_level = get_fpn_location_coords(fpn_feats_shapes, backbone.fpn_strides)\n",
        "\n",
        "# First five location co-ordinates for each feature maps.\n",
        "\n",
        "expected_locations = {\n",
        "    \"p3\": torch.tensor([[4.0, 4.0], [12.0, 4.0], [20.0, 4.0], [28.0, 4.0], [36.0, 4.0]]),\n",
        "    \"p4\": torch.tensor([[8.0, 8.0], [24.0, 8.0], [40.0, 8.0], [56.0, 8.0], [72.0, 8.0]]),\n",
        "    \"p5\": torch.tensor([[16.0, 16.0], [48.0, 16.0], [80.0, 16.0], [112.0, 16.0], [144.0, 16.0]]),\n",
        "}\n",
        "print(\"First five locations per FPN level (absolute image co-ordinates):\")\n",
        "for level_name, locations in locations_per_fpn_level.items():\n",
        "    print(f\"{level_name}: {locations[:5, :].tolist()}\")\n",
        "    print(\"rel error: \", rel_error(expected_locations[level_name], locations[:5, :]))\n",
        "\n",
        "# Visualize all the locations on first image from training data.\n",
        "for level_name, locations in locations_per_fpn_level.items():\n",
        "    # Un-normalize image to bring in [0, 1] RGB range.\n",
        "    image = inverse_norm(val_dataset[0][1])\n",
        "\n",
        "    print(\"*\" * 80)\n",
        "    print(f\"All locations of the image FPN level = {level_name}\")\n",
        "    print(f\"stride = {backbone.fpn_strides[level_name]}\")\n",
        "    detection_visualizer(image, val_dataset.idx_to_class, points=locations.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** We will use \"feature map location\" and \"feature center\" interchangeably from now on, they mean the same thing — center of the receptive field of a particular feature map location at any FPN level **(green points above)**."
      ],
      "metadata": {
        "id": "-367rRuDqkKn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yla_kV-CKPIY"
      },
      "source": [
        "### 7.1 (e) Matching feature map locations with GT boxes\n",
        "\n",
        "Now we match these locations with GT boxes for supervising our network. FCOS matches some `N` locations at any given FPN level with `M` GT boxes applying two rules:\n",
        "\n",
        "> Location $N_i$ is matched with box $M_i$ if it lies inside the box. If any location lies inside two boxes, then it is matched with the smaller box. If a location does not lie inside any box, it is assigned \"background\".\n",
        "\n",
        "> _Multi-scale matching_ for different FPN levels — for a particular FPN level, FCOS only considers a subset of boxes based on their size. Intuitively, larger boxes are assigned to `p5` and smaller boxes are assigned to `p3`.\n",
        "\n",
        "As a result of this matching, each location wil receive a bounding box and a class label (that is 5D vector `(x1, y1, x2, y2, C)`) out of `M` GT boxes, or a background `(-1, -1, -1, -1, -1)`.\n",
        "\n",
        "We have implemented this matching procedure for you, because we thought it is non-trivial for the limited time and difficulty level of this assignment.\n",
        "However, you are required to understand its input/output format and how to use it, shown in the following cell.\n",
        "We recommend you to read its implementation in `ps7_helpers.py` with name `fcos_match_locations_to_gt`. While not required for this assignment, you may refer to [Section 3.2 in FCOS paper](https://arxiv.org/abs/1904.01355)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEDkc2SPgKpn"
      },
      "outputs": [],
      "source": [
        "from ps7_helpers import fcos_match_locations_to_gt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQRazIjUKeQx"
      },
      "outputs": [],
      "source": [
        "# Get an image and its GT boxes from train dataset.\n",
        "#try with different images using index 1,2,3...\n",
        "_, image, gt_boxes = train_dataset[0]\n",
        "\n",
        "# Dictionary with keys {\"p3\", \"p4\", \"p5\"} and values as `(N, 5)` tensors\n",
        "# giving matched GT boxes.\n",
        "matched_boxes_per_fpn_level = fcos_match_locations_to_gt(\n",
        "    locations_per_fpn_level, backbone.fpn_strides, gt_boxes\n",
        ")\n",
        "\n",
        "# Visualize one selected location (yellow point) and its matched GT box (red).\n",
        "# Get indices of matched locations (whose class ID is not -1) from P3 level.\n",
        "FPN_LEVEL = \"p4\"\n",
        "# NOTE: Run this cell multiple times with FPN_LEVEL = p4,p5 to see different matched points. \n",
        "# For car image, p3/5 will not work because the one and only box was already assigned\n",
        "# to p4 due to its compatible size to p4 stride. Similarly for some other images\n",
        "\n",
        "fg_idxs_p3 = (matched_boxes_per_fpn_level[FPN_LEVEL][:, 4] != -1).nonzero()\n",
        "\n",
        "_idx = random.choice(fg_idxs_p3)\n",
        "\n",
        "detection_visualizer(\n",
        "    inverse_norm(image),\n",
        "    val_dataset.idx_to_class,\n",
        "    bbox=matched_boxes_per_fpn_level[FPN_LEVEL][_idx],\n",
        "    points=locations_per_fpn_level[FPN_LEVEL][_idx]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "546xmZj_LJFF"
      },
      "source": [
        "In the above illustration, we can see one random location on `p3` level (**yellow dot**), and its matched GT box (**red**).\n",
        "With these GT targets assigned, the FCOS preiction heads are task to predict this class label (**person**) at the given location, regress the distances between this location to all box edges `(left, top, right, bottom)`, and regress a real-valued centerness at this location. \n",
        "\n",
        "From now on, it would help to think about FCOS behavior for a _single_ location, assuming that it has a matched GT box (or background). We shall now discuss and implement the output format of prediction layers and loss functions during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azEQXzEWLMb2"
      },
      "source": [
        "### 7.1 (f) GT Targets for box regression\n",
        "The box regression head is tasked with predicting **FOUR** values which are the distances from feature locations (yellow dots) to box edges (red box) as we discussed above (left, top, right, bottom) or simply as we could call it — LTRB. \n",
        "\n",
        "Recall that all locations and GT boxes so far are represented in absolute image co-ordinates — they range from (0, 224) for our input image, and can be even larger for abritrarily large input images.\n",
        "\n",
        "We cannot use this absolute co-ordinate format with our network because regressing to such large real-valued numbers will cause the gradients to explode. Hence, FCOS normalizes LTRB targets by the stride of FPN levels. Hence in the above example, consider FPN level P3 (stride = 8), the location to be (xc, yc) (yellow point) and matched GT box have co-ordinates `(x1, y1, x2, y2)`. Then the LTRB regression targets used to supervise the network are:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "l = (xc - x1) / stride              t = (yc - y1) / stride\n",
        "r = (x2 - xc) / stride              b = (y2 - yc) / stride\n",
        "```\n",
        "\n",
        "\n",
        "These are commonly referred as \"deltas\" of box regression. Since the model is supervised to predict these during training time, one must apply an inverse transformation to these during inference to convert network outputs to predicted boxes in absolute image co-ordinates.\n",
        "\n",
        "Next, you will implement this transformation logic and its inverse in two separate functions:\n",
        "\n",
        "`fcos_get_deltas_from_locations` : Accepts locations (centers) and GT boxes, and returns deltas. Required for training supervision.\n",
        "\n",
        "`fcos_apply_deltas_to_locations`: Accepts predicted deltas and locations, and returns predicted boxes. Required during inference.\n",
        "Run the following cell to check your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG5GmAS_genA"
      },
      "outputs": [],
      "source": [
        "def fcos_get_deltas_from_locations(\n",
        "    locations: torch.Tensor, gt_boxes: torch.Tensor, stride: int\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute distances from feature locations to GT box edges. These distances\n",
        "    are called \"deltas\" - `(left, top, right, bottom)` or simply `LTRB`. The\n",
        "    feature locations and GT boxes are given in absolute image co-ordinates.\n",
        "\n",
        "    These deltas are used as targets for training FCOS to perform box regression\n",
        "    and centerness regression. They must be \"normalized\" by the stride of FPN\n",
        "    feature map (from which feature locations were computed, see the function\n",
        "    `get_fpn_location_coords`). If GT boxes are \"background\", then deltas must\n",
        "    be `(-1, -1, -1, -1)`.\n",
        "\n",
        "    NOTE: This transformation function should not require GT class label. Your\n",
        "    implementation must work for GT boxes being `(N, 4)` or `(N, 5)` tensors -\n",
        "    without or with class labels respectively. You may assume that all the\n",
        "    background boxes will be `(-1, -1, -1, -1)` or `(-1, -1, -1, -1, -1)`.\n",
        "\n",
        "    Args:\n",
        "        locations: Tensor of shape `(N, 2)` giving `(xc, yc)` feature locations.\n",
        "        gt_boxes: Tensor of shape `(N, 4 or 5)` giving GT boxes.\n",
        "        stride: Stride of the FPN feature map.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor\n",
        "            Tensor of shape `(N, 4)` giving deltas from feature locations, that\n",
        "            are normalized by feature stride.\n",
        "    \"\"\"\n",
        "    ##########################################################################\n",
        "    # TODO: Implement the logic to get deltas from feature locations.        #\n",
        "    ##########################################################################\n",
        "    # Set this to Tensor of shape (N, 4) giving deltas (left, top, right, bottom)\n",
        "    # from the locations to GT box edges, normalized by FPN stride.\n",
        "    deltas = None\n",
        "\n",
        "    # Replace \"pass\" statement with your code\n",
        "\n",
        "    pass\n",
        "    \n",
        "    ##########################################################################\n",
        "    #                             END OF YOUR CODE                           #\n",
        "    ##########################################################################\n",
        "\n",
        "    return deltas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67yrf-JRgjEZ"
      },
      "outputs": [],
      "source": [
        "def fcos_apply_deltas_to_locations(\n",
        "    deltas: torch.Tensor, locations: torch.Tensor, stride: int\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Implement the inverse of `fcos_get_deltas_from_locations` here:\n",
        "\n",
        "    Given edge deltas (left, top, right, bottom) and feature locations of FPN, get\n",
        "    the resulting bounding box co-ordinates by applying deltas on locations. This\n",
        "    method is used for inference in FCOS: deltas are outputs from model, and\n",
        "    applying them to anchors will give us final box predictions.\n",
        "\n",
        "    Recall in above method, we were required to normalize the deltas by feature\n",
        "    stride. Similarly, we have to un-normalize the input deltas with feature\n",
        "    stride before applying them to locations, because the given input locations are\n",
        "    already absolute co-ordinates in image dimensions.\n",
        "\n",
        "    Args:\n",
        "        deltas: Tensor of shape `(N, 4)` giving edge deltas to apply to locations.\n",
        "        locations: Locations to apply deltas on. shape: `(N, 2)`\n",
        "        stride: Stride of the FPN feature map.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor\n",
        "            Same shape as deltas and locations, giving co-ordinates of the\n",
        "            resulting boxes `(x1, y1, x2, y2)`, absolute in image dimensions.\n",
        "    \"\"\"\n",
        "    ##########################################################################\n",
        "    # TODO: Implement the transformation logic to get boxes.                 #\n",
        "    #                                                                        #\n",
        "    # NOTE: The model predicted deltas MAY BE negative, which is not valid   #\n",
        "    # for our use-case because the feature center must lie INSIDE the final  #\n",
        "    # box. Make sure to clip them to zero.                                   #\n",
        "    ##########################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "\n",
        "    pass\n",
        "    \n",
        "    ##########################################################################\n",
        "    #                             END OF YOUR CODE                           #\n",
        "    ##########################################################################\n",
        "\n",
        "    return output_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GkoUG3XLTXX"
      },
      "outputs": [],
      "source": [
        "# Three hard-coded input boxes and three points lying inside them.\n",
        "# Add a dummy class ID = 1 indicating foreground\n",
        "input_boxes = torch.Tensor(\n",
        "    [[10, 15, 100, 115, 1], [30, 20, 40, 30, 1], [120, 100, 200, 200, 1]]\n",
        ")\n",
        "input_locations = torch.Tensor([[30, 40], [32, 29], [125, 150]])\n",
        "\n",
        "# Here we do a simple sanity check - getting deltas for a particular set of boxes\n",
        "# and applying them back to centers should give us the same boxes. Setting a random\n",
        "# stride = 8, it should not affect reconstruction if it is same on both sides.\n",
        "_deltas = fcos_get_deltas_from_locations(input_locations, input_boxes, stride=8)\n",
        "output_boxes = fcos_apply_deltas_to_locations(_deltas, input_locations, stride=8)\n",
        "\n",
        "print(\"Rel error in reconstructed boxes:\", rel_error(input_boxes[:, :4], output_boxes))\n",
        "\n",
        "\n",
        "# Another check: deltas for GT class label = -1 should be -1.\n",
        "background_box = torch.Tensor([[-1, -1, -1, -1, -1]])\n",
        "input_location = torch.Tensor([[100, 200]])\n",
        "\n",
        "_deltas = fcos_get_deltas_from_locations(input_location, background_box, stride=8)\n",
        "output_box = fcos_apply_deltas_to_locations(_deltas, input_location, stride=8)\n",
        "\n",
        "print(\"Background deltas should be all -1    :\", _deltas)\n",
        "\n",
        "# Output box should be the location itself ([100, 200, 100, 200])\n",
        "print(\"Output box with background deltas     :\", output_box)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9_YnkVhLzcb"
      },
      "source": [
        "### 7.1 (g) GT targets for centerness regression\n",
        "\n",
        "Given the GT deltas for a location `(left, top, right, bottom)` as computed above, FCOS defines centerness as:\n",
        "\n",
        "$$centerness = \\sqrt{\\frac{\\min(left, right) \\cdot \\min(top, bottom)}{\\max(left, right) \\cdot \\max(top, bottom)}}$$\n",
        "\n",
        "This value is maximum (1) when `left = right` and `top = bottom`, implying the center of GT box.\n",
        "At the edge of a box, one of these values will be zero, which gives zero centerness at edges.\n",
        "Centerness regression head uses these values as targets.\n",
        "\n",
        "We have implemented this function for you as centerness is not a focus of this problem set. Run the cell below to import the function from `ps7_helpers.py`. If you want, you can open the file and get an understanding of how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfwm4Sa4ggTR"
      },
      "outputs": [],
      "source": [
        "from ps7_helpers import fcos_make_centerness_targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CTcLhwhL78L"
      },
      "source": [
        "### 7.1 (h) Loss Functions\n",
        "\n",
        "At this point, every model prediction is assigned a GT target during training.\n",
        "We will proceed to compute losses for training the model.\n",
        "\n",
        "FCOS has three prediction layers, that use the following use functions:\n",
        "\n",
        "1. **Object classification:** FCOS uses [Sigmoid Focal Loss](https://arxiv.org/abs/1708.02002), an extension of cross-entropy loss that deals with class-imbalance. FCOS faces a class imbalance issue because a majority of locations would be assigned \"background\". If not handled properly, the model will simply learn to predict \"background\" for every location. We will use thee torch implementation of this loss directly using `torchvision.ops.sigmoid_focal_loss`. You can read more about it in the [torch documentation](https://pytorch.org/vision/stable/generated/torchvision.ops.sigmoid_focal_loss.html)\n",
        "\n",
        "2. **Box regression:** The FCOS paper uses [Generalized Intersection-over-Union](https://giou.stanford.edu/) loss to minimize the difference between predicted and GT `LTRB` deltas. You are not required to understand GIoU for this assignment. We will use the Torch implementation of this loss directly using `torchvision.ops.generalized_box_iou_loss`. It is used very similarly to the other Torch losses we have used so far. You can read more about it in the [Torch documentation](https://pytorch.org/vision/stable/generated/torchvision.ops.generalized_box_iou_loss.html)\n",
        "\n",
        "3. **Centerness regression:** Centerness predictions and GT targets are real-valued numbers in `[0, 1]`, so FCOS uses binary cross-entropy (BCE) loss to optimize it. One may use an L1 loss, but BCE empirically works slightly better.\n",
        "\n",
        "**Total loss:** We get three loss components _per location_. Out of these, (2) and (3) are set to zero for _background locations_ because their GT boxes (and hence centerness) are not defined. Total loss is the sum of all losses per location, averaged by the number of _foreground locations_ (that matched with any GT box). The number of foreground locations are highly variable per image, depending on density of objects in it. Hence for training stability, the loss is instead average by an _exponential moving average of foreground locations_ (think like the running mean/var of BN, in past assignment)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have implemented some basic forms of these losses in past assignment.\n",
        "So now you are allowed to directly use builtin functions from PyTorch and Torchvision.\n",
        "We highly encourage you to check out their implementation and understand them closely.\n",
        "\n",
        "The following two cells demonstrate their use with dummy inputs and targets.\n",
        "Execute them and read them carefully so you understand how to call these functions — immediately after, you will use them with actual predictions of FCOS, and assigned GT targets (that you implemented so far)."
      ],
      "metadata": {
        "id": "8gkV6i-B5yq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check: dummy model predictions for TWO locations, and\n",
        "# NUM_CLASSES = 5 (typically there are thousands of locations\n",
        "# across all FPN levels).\n",
        "# shape: (batch_size, num_locations, num_classes)\n",
        "dummy_pred_cls_logits = torch.randn(1, 2, 5)\n",
        "\n",
        "# Corresponding one-hot vectors of GT class labels (2, -1), one\n",
        "# foreground and one background.\n",
        "# shape: (batch_size, num_locations, num_classes)\n",
        "dummy_gt_classes = torch.Tensor([[[0, 0, 1, 0, 0], [0, 0, 0, 0, 0]]])\n",
        "\n",
        "# This loss expects logits, not probabilities (DO NOT apply sigmoid!)\n",
        "cls_loss = sigmoid_focal_loss(\n",
        "    inputs=dummy_pred_cls_logits, targets=dummy_gt_classes\n",
        ")\n",
        "#this value is completely random\n",
        "print(\"Classification loss (dummy inputs/targets):\")\n",
        "print(cls_loss)\n",
        "\n",
        "#this value is completely random\n",
        "print(f\"Total classification loss (un-normalized): {cls_loss.sum()}\")\n",
        "\n",
        "#the purpose of this sanity check is to give you an idea of how to use the different losses"
      ],
      "metadata": {
        "id": "rsPyCH2z5QkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check: dummy model predictions for TWO locations, and\n",
        "# NUM_CLASSES = 2 (typically there are thousands of locations\n",
        "# across all FPN levels).\n",
        "# Think of these as first two locations locations of \"p5\" level.\n",
        "dummy_locations = torch.Tensor([[32, 32], [64, 32]])\n",
        "dummy_gt_boxes = torch.Tensor(\n",
        "    [\n",
        "        [1, 2, 40, 50, 2],\n",
        "        [-1, -1, -1, -1, -1]  # Same GT classes as above cell.\n",
        "    ]\n",
        ")\n",
        "# Centerness is just a dummy value:\n",
        "dummy_gt_centerness = torch.Tensor([0.6, -1])\n",
        "\n",
        "# shape: (batch_size, num_locations, 4 or 1)\n",
        "dummy_pred_boxreg_deltas = torch.randn(1, 2, 4)\n",
        "dummy_pred_ctr_logits = torch.randn(1, 2, 1)\n",
        "\n",
        "# Collapse batch dimension.\n",
        "dummy_pred_boxreg_deltas = dummy_pred_boxreg_deltas.view(-1, 4)\n",
        "dummy_pred_ctr_logits = dummy_pred_ctr_logits.view(-1)\n",
        "\n",
        "dummy_pred_boxes=fcos_apply_deltas_to_locations(dummy_pred_boxreg_deltas,dummy_locations,stride=32)\n",
        "\n",
        "# First calculate box reg loss, comparing predicted boxes and GT boxes.\n",
        "dummy_gt_deltas = fcos_get_deltas_from_locations(\n",
        "    dummy_locations, dummy_gt_boxes, stride=32\n",
        ")\n",
        "\n",
        "loss_box=torchvision.ops.generalized_box_iou_loss(dummy_pred_boxes,dummy_gt_boxes[:,:4],reduction=\"none\")\n",
        "\n",
        "# No loss for background:\n",
        "dummy_gt_boxes_cloned=dummy_gt_boxes[:,4].clone().reshape(-1)\n",
        "bg_indices=dummy_gt_boxes_cloned==-1\n",
        "loss_box[bg_indices] = 0.0\n",
        "print(\"Box regression loss (GIoU):\", loss_box)\n",
        "\n",
        "#loss_box=torchvision.ops.generalized_box_iou_loss(dummy_pred_boxes,dummy_gt_boxes,reduction=\"none\")\n",
        "# Now calculate centerness loss.\n",
        "centerness_loss = F.binary_cross_entropy_with_logits(\n",
        "    dummy_pred_ctr_logits, dummy_gt_centerness, reduction=\"none\"\n",
        ")\n",
        "\n",
        "# No loss for background:\n",
        "centerness_loss[dummy_gt_centerness < 0] *= 0.0\n",
        "print(\"Centerness loss (BCE):\", centerness_loss)\n",
        "\n",
        "# In both the expected losses, the first value will be different everytime due to random dummy\n",
        "# predictions. But the second value should always be zero - corresponding to background."
      ],
      "metadata": {
        "id": "PtfY49BH5eZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmfp13OrMGyk"
      },
      "source": [
        "### 7.1 (i) Object detection module\n",
        "\n",
        "We will now combine everything into the `FCOS` class.\n",
        "Implement the `__init__` and `forward` functions of this module — you have already done most of the heavy lifting, you simply need to call the functions in a correct way!\n",
        "Use the previous two cells as a reference to implement loss calculation in `forward()`.\n",
        "\n",
        "You can come back and implement the `inference` function later after you have successfully trained the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pZrjKljQ-RV"
      },
      "outputs": [],
      "source": [
        "class FCOS(nn.Module):\n",
        "    \"\"\"\n",
        "    FCOS: Fully-Convolutional One-Stage Detector\n",
        "\n",
        "    This class puts together everything you implemented so far. It contains a\n",
        "    backbone with FPN, and prediction layers (head). It computes loss during\n",
        "    training and predicts boxes during inference.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, num_classes: int, fpn_channels: int, stem_channels: List[int]\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        ######################################################################\n",
        "        # TODO: Initialize backbone and prediction network using arguments.  #\n",
        "        ######################################################################\n",
        "        # Feel free to delete these two lines: (but keep variable names same)\n",
        "        self.backbone = None\n",
        "        self.pred_net = None\n",
        "        # Replace \"pass\" statement with your code\n",
        "\n",
        "        pass\n",
        "\n",
        "        ######################################################################\n",
        "        #                           END OF YOUR CODE                         #\n",
        "        ######################################################################\n",
        "\n",
        "        # Averaging factor for training loss; EMA of foreground locations.\n",
        "        # STUDENTS: See its use in `forward` when you implement losses.\n",
        "        self._normalizer = 150  # per image\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        images: torch.Tensor,\n",
        "        gt_boxes: Optional[torch.Tensor] = None,\n",
        "        test_score_thresh: Optional[float] = None,\n",
        "        test_nms_thresh: Optional[float] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images: Batch of images, tensors of shape `(B, C, H, W)`.\n",
        "            gt_boxes: Batch of training boxes, tensors of shape `(B, N, 5)`.\n",
        "                `gt_boxes[i, j] = (x1, y1, x2, y2, C)` gives information about\n",
        "                the `j`th object in `images[i]`. The position of the top-left\n",
        "                corner of the box is `(x1, y1)` and the position of bottom-right\n",
        "                corner of the box is `(x2, x2)`. These coordinates are\n",
        "                real-valued in `[H, W]`. `C` is an integer giving the category\n",
        "                label for this bounding box. Not provided during inference.\n",
        "            test_score_thresh: During inference, discard predictions with a\n",
        "                confidence score less than this value. Ignored during training.\n",
        "            test_nms_thresh: IoU threshold for NMS during inference. Ignored\n",
        "                during training.\n",
        "\n",
        "        Returns:\n",
        "            Losses during training and predictions during inference.\n",
        "        \"\"\"\n",
        "\n",
        "        ######################################################################\n",
        "        # TODO: Process the image through backbone, FPN, and prediction head #\n",
        "        # to obtain model predictions at every FPN location.                 #\n",
        "        # Get dictionaries of keys {\"p3\", \"p4\", \"p5\"} giving predicted class #\n",
        "        # logits, deltas, and centerness.                                    #\n",
        "        ######################################################################\n",
        "        # Feel free to delete this line: (but keep variable names same)\n",
        "        pred_cls_logits, pred_boxreg_deltas, pred_ctr_logits = None, None, None\n",
        "        # Replace \"pass\" statement with your code\n",
        "\n",
        "        pass\n",
        "\n",
        "        ######################################################################\n",
        "        # TODO: Get absolute co-ordinates `(xc, yc)` for every location in\n",
        "        # FPN levels.\n",
        "        #\n",
        "        # HINT: You have already implemented everything, just have to\n",
        "        # call the functions properly.\n",
        "        ######################################################################\n",
        "        # Feel free to delete this line: (but keep variable names same)\n",
        "        locations_per_fpn_level = None\n",
        "        # Replace \"pass\" statement with your code\n",
        "\n",
        "        pass\n",
        "\n",
        "        ######################################################################\n",
        "        #                           END OF YOUR CODE                         #\n",
        "        ######################################################################\n",
        "\n",
        "        if not self.training:\n",
        "            # During inference, just go to this method and skip rest of the\n",
        "            # forward pass.\n",
        "            # fmt: off\n",
        "            return self.inference(\n",
        "                images, locations_per_fpn_level,\n",
        "                pred_cls_logits, pred_boxreg_deltas, pred_ctr_logits,\n",
        "                test_score_thresh=test_score_thresh,\n",
        "                test_nms_thresh=test_nms_thresh,\n",
        "            )\n",
        "            # fmt: on\n",
        "\n",
        "        ######################################################################\n",
        "        # TODO: Assign ground-truth boxes to feature locations. We have this\n",
        "        # implemented in a `fcos_match_locations_to_gt`. \n",
        "        #Find the predicted boxes (to be used to calculate the box loss) for each feature location using the predicted boxreg deltas\n",
        "        #Both the operations are NOT\n",
        "        # batched so call it separately per GT boxes / predicted boxes in batch.\n",
        "        ######################################################################\n",
        "        \n",
        "        # List of dictionaries with keys {\"p3\", \"p4\", \"p5\"} giving matched\n",
        "        # boxes for locations per FPN level, per image. Fill this list:\n",
        "        \n",
        "        matched_gt_boxes = []\n",
        "        # Replace \"pass\" statement with your code\n",
        "\n",
        "        pass\n",
        "\n",
        "        # Calculate GT deltas for these matched boxes. Similar structure\n",
        "        # as `matched_gt_boxes` above. Fill this list:\n",
        "        matched_gt_deltas = []\n",
        "        \n",
        "        # Replace \"pass\" statement with your code\n",
        "\n",
        "        pass\n",
        "  \n",
        "        ######################################################################\n",
        "        #                           END OF YOUR CODE                         #\n",
        "        ######################################################################\n",
        "\n",
        "        # Collate lists of dictionaries, to dictionaries of batched tensors.\n",
        "        # These are dictionaries with keys {\"p3\", \"p4\", \"p5\"} and values as\n",
        "        # tensors of shape (batch_size, locations_per_fpn_level, 5 or 4)\n",
        "        matched_gt_boxes = default_collate(matched_gt_boxes)\n",
        "        matched_gt_deltas = default_collate(matched_gt_deltas)\n",
        "        pred_boxes= default_collate(pred_boxes)\n",
        "\n",
        "        # Combine predictions and GT from across all FPN levels.\n",
        "        # shape: (batch_size, num_locations_across_fpn_levels, ...)\n",
        "        matched_gt_boxes = self._cat_across_fpn_levels(matched_gt_boxes)\n",
        "        matched_gt_deltas = self._cat_across_fpn_levels(matched_gt_deltas)\n",
        "        pred_cls_logits = self._cat_across_fpn_levels(pred_cls_logits)\n",
        "        pred_boxreg_deltas = self._cat_across_fpn_levels(pred_boxreg_deltas)\n",
        "        pred_ctr_logits = self._cat_across_fpn_levels(pred_ctr_logits)\n",
        "        pred_boxes = self._cat_across_fpn_levels(pred_boxes)\n",
        "\n",
        "        # Perform EMA update of normalizer by number of positive locations.\n",
        "        num_pos_locations = (matched_gt_boxes[:, :, 4] != -1).sum()\n",
        "        pos_loc_per_image = num_pos_locations.item() / images.shape[0]\n",
        "        self._normalizer = 0.9 * self._normalizer + 0.1 * pos_loc_per_image\n",
        "\n",
        "        #######################################################################\n",
        "        # TODO: Calculate losses per location for classification, box reg and\n",
        "        # centerness. Remember to set box/centerness losses for \"background\"\n",
        "        # positions to zero.\n",
        "        ######################################################################\n",
        "        # Feel free to delete this line: (but keep variable names same)\n",
        "        loss_cls, loss_box, loss_ctr = None, None, None\n",
        "\n",
        "        # Replace \"pass\" statement with your code\n",
        "\n",
        "        pass\n",
        "\n",
        "        # Hint for box  loss:\n",
        "        # Find the background images       \n",
        "        # Calculate the box loss        \n",
        "        # Do not count the loss of background images \n",
        "\n",
        "        ######################################################################\n",
        "        #                            END OF YOUR CODE                        #\n",
        "        ######################################################################\n",
        "        \n",
        "        # Sum all locations and average by the EMA of foreground locations.\n",
        "        # In training code, we simply add these three and call `.backward()`\n",
        "        return {\n",
        "            \"loss_cls\": loss_cls.sum() / (self._normalizer * images.shape[0]),\n",
        "            \"loss_box\": loss_box.sum() / (self._normalizer * images.shape[0]),\n",
        "            \"loss_ctr\": loss_ctr.sum() / (self._normalizer * images.shape[0]),\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def _cat_across_fpn_levels(\n",
        "        dict_with_fpn_levels: Dict[str, torch.Tensor], dim: int = 1\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Convert a dict of tensors across FPN levels {\"p3\", \"p4\", \"p5\"} to a\n",
        "        single tensor. Values could be anything - batches of image features,\n",
        "        GT targets, etc.\n",
        "        \"\"\"\n",
        "        return torch.cat(list(dict_with_fpn_levels.values()), dim=dim)\n",
        "\n",
        "    \n",
        "\n",
        "    def inference(\n",
        "        self,\n",
        "        images: torch.Tensor,\n",
        "        locations_per_fpn_level: Dict[str, torch.Tensor],\n",
        "        pred_cls_logits: Dict[str, torch.Tensor],\n",
        "        pred_boxreg_deltas: Dict[str, torch.Tensor],\n",
        "        pred_ctr_logits: Dict[str, torch.Tensor],\n",
        "        test_score_thresh: float = 0.3,\n",
        "        test_nms_thresh: float = 0.5,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Run inference on a single input image (batch size = 1). Other input\n",
        "        arguments are same as those computed in `forward` method. This method\n",
        "        should not be called from anywhere except from inside `forward`.\n",
        "\n",
        "        Returns:\n",
        "            Three tensors:\n",
        "                - pred_boxes: Tensor of shape `(N, 4)` giving *absolute* XYXY\n",
        "                  co-ordinates of predicted boxes.\n",
        "\n",
        "                - pred_classes: Tensor of shape `(N, )` giving predicted class\n",
        "                  labels for these boxes (one of `num_classes` labels). Make\n",
        "                  sure there are no background predictions (-1).\n",
        "\n",
        "                - pred_scores: Tensor of shape `(N, )` giving confidence scores\n",
        "                  for predictions: these values are `sqrt(class_prob * ctrness)`\n",
        "                  where class_prob and ctrness are obtained by applying sigmoid\n",
        "                  to corresponding logits.\n",
        "        \"\"\"\n",
        "\n",
        "        # Gather scores and boxes from all FPN levels in this list. Once\n",
        "        # gathered, we will perform NMS to filter highly overlapping predictions.\n",
        "        pred_boxes_all_levels = []\n",
        "        pred_classes_all_levels = []\n",
        "        pred_scores_all_levels = []\n",
        "\n",
        "        for level_name in locations_per_fpn_level.keys():\n",
        "\n",
        "            # Get locations and predictions from a single level.\n",
        "            # We index predictions by `[0]` to remove batch dimension.\n",
        "            level_locations = locations_per_fpn_level[level_name]\n",
        "            level_cls_logits = pred_cls_logits[level_name][0]\n",
        "            level_deltas = pred_boxreg_deltas[level_name][0]\n",
        "            level_ctr_logits = pred_ctr_logits[level_name][0]\n",
        "\n",
        "            ##################################################################\n",
        "            # TODO: FCOS uses the geometric mean of class probability and\n",
        "            # centerness as the final confidence score. This helps in getting\n",
        "            # rid of excessive amount of boxes far away from object centers.\n",
        "            # Compute this value here (recall sigmoid(logits) = probabilities)\n",
        "            #\n",
        "            # Then perform the following steps in order:\n",
        "            #   1. Get the most confidently predicted class and its score for\n",
        "            #      every box. Use level_pred_scores: (N, num_classes) => (N, )\n",
        "            #   2. Only retain prediction that have a confidence score higher\n",
        "            #      than provided threshold in arguments.\n",
        "            #   3. Obtain predicted boxes using predicted deltas and locations\n",
        "            #   4. Clip XYXY box-cordinates that go beyond thr height and\n",
        "            #      and width of input image.\n",
        "            ##################################################################\n",
        "            # Feel free to delete this line: (but keep variable names same)\n",
        "            level_pred_boxes, level_pred_classes, level_pred_scores = (\n",
        "                None,\n",
        "                None,\n",
        "                None,  # Need tensors of shape: (N, 4) (N, ) (N, )\n",
        "            )\n",
        "\n",
        "            # Compute geometric mean of class logits and centerness:\n",
        "            level_pred_scores = torch.sqrt(\n",
        "                level_cls_logits.sigmoid_() * level_ctr_logits.sigmoid_()\n",
        "            )\n",
        "            # Step 1:\n",
        "            # Replace \"pass\" statement with your code\n",
        "            pass\n",
        "\n",
        "            # Step 2:\n",
        "            # Replace \"pass\" statement with your code\n",
        "            pass\n",
        "\n",
        "            # Step 3:\n",
        "            # Replace \"pass\" statement with your code\n",
        "            pass\n",
        "\n",
        "            # Step 4: Use `images` to get (height, width) for clipping.\n",
        "            # Replace \"pass\" statement with your code\n",
        "            pass\n",
        "\n",
        "            # Step 5: Clip \n",
        "            pass\n",
        "\n",
        "            ##################################################################\n",
        "            #                          END OF YOUR CODE                      #\n",
        "            ##################################################################\n",
        "\n",
        "            pred_boxes_all_levels.append(level_pred_boxes)\n",
        "            pred_classes_all_levels.append(level_pred_classes)\n",
        "            pred_scores_all_levels.append(level_pred_scores)\n",
        "\n",
        "        ######################################################################\n",
        "        # Combine predictions from all levels and perform NMS.\n",
        "        pred_boxes_all_levels = torch.cat(pred_boxes_all_levels)\n",
        "        pred_classes_all_levels = torch.cat(pred_classes_all_levels)\n",
        "        pred_scores_all_levels = torch.cat(pred_scores_all_levels)\n",
        "\n",
        "        # STUDENTS: This function depends on your implementation of NMS.\n",
        "        keep = class_spec_nms(\n",
        "            pred_boxes_all_levels,\n",
        "            pred_scores_all_levels,\n",
        "            pred_classes_all_levels,\n",
        "            iou_threshold=test_nms_thresh,\n",
        "        )\n",
        "        pred_boxes_all_levels = pred_boxes_all_levels[keep]\n",
        "        pred_classes_all_levels = pred_classes_all_levels[keep]\n",
        "        pred_scores_all_levels = pred_scores_all_levels[keep]\n",
        "        return (\n",
        "            pred_boxes_all_levels,\n",
        "            pred_classes_all_levels,\n",
        "            pred_scores_all_levels,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PSu7aVHMK4_"
      },
      "source": [
        "## 7.1 (j) Overfit small data\n",
        "\n",
        "We have implemented the `train_detector` function which runs the training loop for this FCOS detector. You can read its implementation in `ps7_helpers.py`. \n",
        "\n",
        "To make sure that everything is working as expected, we can try to overfit the detector to a small subset of data.\n",
        "\n",
        "**NOTE:** The training loss may start low and end up higher at the end of 500 iterations, that is fine. The main training loop later will work better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3_imRqtRKPK"
      },
      "outputs": [],
      "source": [
        "reset_seed()\n",
        "\n",
        "detector = FCOS(\n",
        "    num_classes=NUM_CLASSES,\n",
        "    fpn_channels=64,\n",
        "    stem_channels=[64,64],\n",
        ")\n",
        "detector = detector.to(DEVICE)\n",
        "\n",
        "train_detector(\n",
        "    detector,\n",
        "    train_loader,\n",
        "    learning_rate=5e-3,\n",
        "    max_iters=500,\n",
        "    log_period=20,\n",
        "    device=DEVICE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3AumjO_MyQx"
      },
      "source": [
        "## 7.1 (j) Train a net\n",
        "\n",
        "Now that we are confident that the training code is working properly, let's train the network on more data and for longer. We will train for 9000 iterations; this should take about 35-40 minutes on a Tesla T4 GPU. For initial debugging, you may want to train for lesser durations (say 1000 iterations). The loss should start at around 1.3, increase for some time and settle at around 0.5-0.8.\n",
        "\n",
        "Note that real object detection systems typically train for 12-24 hours, distribute training over multiple GPUs, and use much faster GPUs. As such our result will be far from the state of the art, but it should give some reasonable results!\n",
        "\n",
        "**NOTE:** Pay close attention to the training for initial 1000 iterations, ometimes you might see nan values in the training loss. We have not been able to pinpoint a single cause for this. The best solution we have found for this is to just interrupt the cell execution and run it again. Take note that this may also happen due to coding errors in your case. If the overfit training also shows nan then the problem is most likely in your code."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check which GPU you have\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "ixHhsdZ_u9O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSI89WlnM0qM"
      },
      "outputs": [],
      "source": [
        "reset_seed()\n",
        "\n",
        "# Slightly larger detector than in above cell.\n",
        "detector = FCOS(\n",
        "    num_classes=NUM_CLASSES,\n",
        "    fpn_channels=128,\n",
        "    stem_channels=[128,128],\n",
        ")\n",
        "detector = detector.to(DEVICE)\n",
        "\n",
        "train_detector(\n",
        "    detector,\n",
        "    train_loader,\n",
        "    learning_rate=5e-3,\n",
        "    max_iters=9000,\n",
        "    log_period=100,\n",
        "    device=DEVICE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save your trained detector weights"
      ],
      "metadata": {
        "id": "NEmqA9EjuT2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(detector.state_dict(), \"fcos_detector.pt\")"
      ],
      "metadata": {
        "id": "iQ1GuF4czxTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjyRsHLLOs5A"
      },
      "source": [
        "## 7.1 (k) Inference\n",
        "\n",
        "Now, we will use the trained model to perform inference on the val dataset. We have implemented a helper function for you that makes use of the `inference` function you implemented in `FCOS`. Just run the cell below to get the results.\n",
        "\n",
        "Visualize the output from the trained model on a few eval images by running the code below, the bounding boxes should be somewhat accurate. They would get even better by using a bigger model and training it for longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7nxkVPfOuyB"
      },
      "outputs": [],
      "source": [
        "from ps7_helpers import inference_with_detector, class_spec_nms\n",
        "\n",
        "weights_path = \"fcos_detector.pt\"\n",
        "\n",
        "# Re-initialize so this cell is independent from prior cells.\n",
        "detector = FCOS(\n",
        "    num_classes=NUM_CLASSES, fpn_channels=128, stem_channels=[128, 128]\n",
        ")\n",
        "detector.to(device=DEVICE)\n",
        "detector.load_state_dict(torch.load(weights_path, map_location=\"cpu\"))\n",
        "\n",
        "# Prepare a small val daataset for inference:\n",
        "small_dataset = torch.utils.data.Subset(\n",
        "    val_dataset,\n",
        "    torch.linspace(0, len(val_dataset) - 1, steps=10).long()\n",
        ")\n",
        "small_val_loader = torch.utils.data.DataLoader(\n",
        "    small_dataset, batch_size=1, pin_memory=True, num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "image = None\n",
        "inference_with_detector(\n",
        "    detector,\n",
        "    small_val_loader,\n",
        "    val_dataset.idx_to_class,\n",
        "    score_thresh=0.5,\n",
        "    nms_thresh=0.5,\n",
        "    device=DEVICE,\n",
        "    dtype=torch.float32,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szy3X-BDPDDX"
      },
      "source": [
        "## 7.1 (l) Evaluation\n",
        "**Compute mean Average Precision (mAP)**. \n",
        "For an introduction on mAP see *lecture 11* slides.\n",
        "Run the following to evaluate your detector on the PASCAL VOC validation set. Evaluation should take a few minutes, and with default hyperparameters declared above, you should see at least 40% mAP.\n",
        "\n",
        "The state of the art on this dataset is >80% mAP! To achieve these results we would need to use a much bigger network, and train with more data and for much longer, but that is beyond the scope of this assigment. **Optional:** For better mAP, you may use more conv layers in head stem, train for 25K+ iterations, and use ResNet-50/RegNet-4GF models in backbone.\n",
        "But make sure you revert the code back for grading purposes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for mAP evaluation\n",
        "!rm -rf mAP\n",
        "!git clone https://github.com/Cartucho/mAP.git\n",
        "!rm -rf mAP/input/*"
      ],
      "metadata": {
        "id": "VfuthfA51LwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wn-1_fr1PJTv"
      },
      "outputs": [],
      "source": [
        "inference_with_detector(\n",
        "    detector,\n",
        "    val_loader,\n",
        "    val_dataset.idx_to_class,\n",
        "    score_thresh=0.4,\n",
        "    nms_thresh=0.6,\n",
        "    device=DEVICE,\n",
        "    dtype=torch.float32,\n",
        "    output_dir=\"mAP/input\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd mAP && python main.py"
      ],
      "metadata": {
        "id": "B5fjeBXcCHzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(filename=\"./mAP/output/mAP.png\")"
      ],
      "metadata": {
        "id": "SFEDbOzBAnwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Notebook to PDF\n",
        "\n",
        "[Alternative if the cell below doesn't work.](https://docs.google.com/document/d/1QTutnoApRow8cOxNrKK6ISEkA72QGfwLFXbIcpvarAI/edit?usp=sharing)"
      ],
      "metadata": {
        "id": "ljErEphNDICj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "drive_mount_point = '/content/drive/'\n",
        "drive.mount(drive_mount_point)"
      ],
      "metadata": {
        "id": "tUWagyf-DI-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### INSTRUCTIONS: \n",
        "\n",
        "\n",
        "*   Please delete the text cells titled \"```***Please delete this cell before pdf conversion***```\" before running the next cell. \n",
        "*   Ensure that you have changed runtime to CPU before conversion\n",
        "\n",
        "\n",
        "\n",
        "Not following the above will cause an error in pdf conversion."
      ],
      "metadata": {
        "id": "n-GdDWFHc0we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate pdf\n",
        "# Please provide the full path of the notebook file below\n",
        "# Important: make sure that your file name does not contain spaces!\n",
        "\n",
        "# Ex: notebookpath = '/content/drive/My Drive/Colab Notebooks/EECS_442_PS4_FA_2022_Starter_Code.ipynb'\n",
        "notebookpath = \"***\"\n",
        "\n",
        "file_name = notebookpath.split('/')[-1]\n",
        "get_ipython().system(\"apt update && apt install texlive-xetex texlive-fonts-recommended texlive-generic-recommended\")\n",
        "get_ipython().system(\"jupyter nbconvert --to PDF {}\".format(notebookpath.replace(' ', '\\\\ ')))\n",
        "files.download(notebookpath.split('.')[0]+'.pdf')"
      ],
      "metadata": {
        "id": "w1_KdfgEDNxi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cCsHXOHhFF_2",
        "2tsOvXUeJW-y",
        "FyCYKTXEJuFa",
        "U9zdrOAUKESg",
        "Yla_kV-CKPIY",
        "azEQXzEWLMb2",
        "N9_YnkVhLzcb",
        "1CTcLhwhL78L",
        "kmfp13OrMGyk",
        "9PSu7aVHMK4_",
        "j3AumjO_MyQx",
        "vjyRsHLLOs5A",
        "Szy3X-BDPDDX",
        "ljErEphNDICj"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}